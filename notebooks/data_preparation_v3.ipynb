{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "219d4fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from fredapi import Fred\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a3df336a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# START_DATE = '2000-01-01'\n",
    "START_DATE = '1990-01-01'  # earliest VIX data: gain ~2000 observations\n",
    "# END_DATE = '2024-09-01'\n",
    "\n",
    "# FRED series mapping\n",
    "FRED_SERIES = {\n",
    "    'VIX': 'VIXCLS',                    # VIX Volatility Index\n",
    "    'Treasury_10Y': 'DGS10',            # 10-Year Treasury Rate\n",
    "    'Yield_Spread': 'T10Y2Y',           # 10Y-2Y Yield Spread (daily, FRED-calculated, no lag adjustment needed)\n",
    "    'CPI': 'CPIAUCSL',                  # Consumer Price Index\n",
    "    'Unemployment': 'UNRATE',           # Unemployment Rate\n",
    "    'Fed_Rate': 'FEDFUNDS',             # Federal Funds Rate\n",
    "    'Consumer_Sentiment': 'UMCSENT',    # Consumer Sentiment Index\n",
    "    # 'Credit_HY': 'BAMLH0A0HYM2',        # High yield credit spread\n",
    "    # 'Credit_IG': 'BAMLC0A0CM'           # Investment grade credit spread\n",
    "    # 'TB3MS': 'TB3MS',                   # for 10Y-3M yield spread - redundant with 10-2Y; removed\n",
    "    # 'PCE': 'PCEPI',                     # alternative inflation measure - highly correlated with CPI; removed\n",
    "    # 'GDP': 'GDP',                       # problematic. quarterly updates. vintage alignment complex; removed\n",
    "    'Industrial_Production': 'INDPRO',  # Industrial Production Index\n",
    "}\n",
    "\n",
    "# Yahoo Finance tickers mapping\n",
    "YAHOO_TICKERS = {\n",
    "    # Volatility measures\n",
    "    # 'VVIX': '^VVIX',                    # Volatility of VIX\n",
    "    \n",
    "    # Sector ETFs (Select Sector SPDRs)\n",
    "    # 'Sector_Tech': 'XLK',               # Technology\n",
    "    # 'Sector_Financials': 'XLF',         # Financials\n",
    "    # 'Sector_Energy': 'XLE',             # Energy\n",
    "    # 'Sector_Healthcare': 'XLV',         # Healthcare\n",
    "    # 'Sector_ConsumerDiscretionary': 'XLY',  # Consumer Discretionary\n",
    "    # 'Sector_ConsumerStaples': 'XLP',    # Consumer Staples\n",
    "    # 'Sector_Industrials': 'XLI',        # Industrials\n",
    "    # 'Sector_Materials': 'XLB',          # Materials\n",
    "    # 'Sector_Utilities': 'XLU',          # Utilities\n",
    "    # 'Sector_RealEstate': 'XLRE',        # Real Estate: began in 2015\n",
    "    # 'Sector_Communications': 'XLC',     # Communication Services: began in 2018\n",
    "    \n",
    "    # Broad market measures\n",
    "    'Wilshire5000': '^W5000',           # Wilshire 5000 Total Market Index\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "419dce0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FRED_API_KEY = \"c8d5b4c26407e7cbfcecca702e0e7aee\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d2328faa-53e6-4797-8424-b7efac43c152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# release date alignment method\n",
    "USE_VINTAGE_DATES = False   # set True to use ALFRED, False for fixed shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "726ab9a9-6465-4621-a7af-c45835444f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_section(title, char='='):\n",
    "    \"\"\"print a formatted section header\"\"\"\n",
    "    print(f'\\n{char * 70}')\n",
    "    print(f'{title}')\n",
    "    print(f'{char * 70}\\n')\n",
    "\n",
    "def print_subsection(title):\n",
    "    \"\"\"print a formatted subsection header\"\"\"\n",
    "    print(f'\\n{title}')\n",
    "    print(f'{\"-\" * 70}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4ab73eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_market_data(start_date=START_DATE,):\n",
    "    \"\"\"Collect S&P 500 market data from Yahoo Finance\"\"\"\n",
    "    print_subsection('Collecting S&P 500 Market Data')\n",
    "\n",
    "    # Download S&P 500 data\n",
    "    sp500 = yf.download(\"^GSPC\", start=start_date,  progress=False)\n",
    "\n",
    "    # Handle MultiIndex columns if present\n",
    "    if hasattr(sp500.columns, 'nlevels') and sp500.columns.nlevels > 1:\n",
    "        sp500.columns = [col[0] for col in sp500.columns]\n",
    "\n",
    "    # Calculate daily returns (percentage)\n",
    "    sp500['Returns'] = sp500['Close'].pct_change() * 100\n",
    "\n",
    "    # Keep only essential columns\n",
    "    market_data = sp500[['Close', 'Volume', 'Returns']].copy()\n",
    "    market_data.columns = ['SP500_Close', 'SP500_Volume', 'SP500_Returns']\n",
    "\n",
    "    print(f'   Collected {len(market_data)} days of S&P 500 data')\n",
    "    print(f'   Date range: {market_data.index[0]} to {market_data.index[-1]}')\n",
    "    return market_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9a8b54f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_fred_data(api_key, start_date=START_DATE):\n",
    "    \"\"\"Collect macroeconomic data from FRED\"\"\"\n",
    "    print_subsection('Collecting FRED Macroeconomic Data')\n",
    "\n",
    "    fred = Fred(api_key=api_key)\n",
    "    fred_data = {}\n",
    "\n",
    "    for name, series_id in FRED_SERIES.items():\n",
    "        try:\n",
    "            data = fred.get_series(series_id, start=start_date)\n",
    "            fred_data[name] = data\n",
    "            print(f\"   ✓ {name}: {len(data)} observations\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ✗ {name}: Error - {str(e)}\")\n",
    "            fred_data[name] = None\n",
    "\n",
    "    print(f'\\n   Summary: Successfully collected {sum(1 for v in fred_data.values() if v is not None)}/{len(FRED_SERIES)} series')\n",
    "    return fred_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aee7ec1b-2da2-4bd4-b755-0c2d82580cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_yahoo_data(tickers, start_date=START_DATE):\n",
    "    \"\"\"\n",
    "    collect data from Yahoo finance for multiple tickers\n",
    "    \n",
    "    params\n",
    "    -----------\n",
    "    tickers : dict\n",
    "        dict mapping feature names toy ahoo ticker symbols\n",
    "        example: {'VVIX': '^VVIX', 'Tech_Sector': 'XLK'}\n",
    "    start_date : str\n",
    "        start date for data collection\n",
    "        \n",
    "    retruns\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        dataframe with all yahoo-sourced features, indexed by date\n",
    "    \"\"\"\n",
    "    print_subsection('Collecting Yahoo Finance Data')\n",
    "    \n",
    "    yahoo_data = {}\n",
    "    \n",
    "    for name, ticker in tickers.items():\n",
    "        try:\n",
    "            data = yf.download(ticker, start=start_date, progress=False)\n",
    "            \n",
    "            # Handle MultiIndex columns if present\n",
    "            if hasattr(data.columns, 'nlevels') and data.columns.nlevels > 1:\n",
    "                data.columns = [col[0] for col in data.columns]\n",
    "            \n",
    "            # Keep only Close price (can add Volume later if needed)\n",
    "            yahoo_data[name] = data['Close']\n",
    "            print(f\"   ✓ {name} ({ticker}): {len(data)} observations\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ✗ {name} ({ticker}): Error - {str(e)}\")\n",
    "            yahoo_data[name] = None\n",
    "    \n",
    "    # Combine into single DataFrame, remove None entries\n",
    "    yahoo_data = {k: v for k, v in yahoo_data.items() if v is not None}\n",
    "    combined = pd.DataFrame(yahoo_data) if yahoo_data else pd.DataFrame()\n",
    "    \n",
    "    print(f'\\n   Summary: Successfully collected {len(yahoo_data)}/{len(tickers)} tickers')\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5b6c3ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_and_combine_data(market_data, fred_data, yahoo_data):\n",
    "    \"\"\"Align different frequency data and combine into master dataset\"\"\"\n",
    "    print(\"\\nAligning and combining data...\")\n",
    "\n",
    "    # Start with market data (daily frequency)\n",
    "    master_df = market_data.copy()\n",
    "\n",
    "    # Add FRED data (forward-fill for non-trading days)\n",
    "    for name, series in fred_data.items():\n",
    "        if series is not None:\n",
    "            # Reindex to match market data dates and forward-fill\n",
    "            aligned_series = series.reindex(master_df.index, method='ffill')\n",
    "            master_df[name] = aligned_series\n",
    "\n",
    "    # add yahoo data if provided\n",
    "    if yahoo_data is not None and not yahoo_data.empty:\n",
    "        for col in yahoo_data.columns:\n",
    "            # Reindex to match market data dates and forward-fill\n",
    "            aligned_series = yahoo_data[col].reindex(master_df.index, method='ffill')\n",
    "            master_df[col] = aligned_series\n",
    "\n",
    "    return master_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8db89a9a-7e4c-4d0e-b8e3-73c1d9b8d725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_quality(df):\n",
    "    '''\n",
    "    Perform comprehensive data quality checks.\n",
    "    '''\n",
    "    print_subsection('Data Quality Check')\n",
    "    \n",
    "    # Check missing values\n",
    "    total_missing = df.isnull().sum().sum()\n",
    "    print(f'Total missing values: {total_missing}')\n",
    "    if total_missing > 0:\n",
    "        print('\\nMissing by column:')\n",
    "        missing_cols = df.isnull().sum()\n",
    "        for col, count in missing_cols[missing_cols > 0].items():\n",
    "            pct = (count / len(df)) * 100\n",
    "            print(f'  {col}: {count} ({pct:.1f}%)')\n",
    "    \n",
    "    # Check for extreme returns (potential data errors)\n",
    "    if 'SP500_Returns' in df.columns:\n",
    "        extreme_returns = df[abs(df['SP500_Returns']) > 10]\n",
    "        print(f'\\nExtreme daily returns (>10%): {len(extreme_returns)}')\n",
    "        if len(extreme_returns) > 0 and len(extreme_returns) < 10:\n",
    "            print('Dates with extreme returns:')\n",
    "            for date, ret in extreme_returns['SP500_Returns'].items():\n",
    "                print(f'  {date.strftime(\"%Y-%m-%d\")}: {ret:.2f}%')\n",
    "    \n",
    "    # Check data types\n",
    "    print('\\nData types:')\n",
    "    print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ba4075d6-434e-492f-803f-2a4d56dfac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_vintage_date_alignment(df, api_key):\n",
    "    '''\n",
    "    apply exact release date alignment using ALFRED vintage dates\n",
    "    \n",
    "    falls back to forward-fill for periods where vintage data unavailable\n",
    "    '''\n",
    "    print_subsection('Applying ALFRED vintage date alignment')\n",
    "    \n",
    "    fred = Fred(api_key=api_key)\n",
    "    df_aligned = df.copy()\n",
    "    \n",
    "    series_mapping = {\n",
    "        'CPI': 'CPIAUCSL',\n",
    "        # 'PCE': 'PCEPI',\n",
    "        # 'TB3MS': 'TB3MS',\n",
    "        'Unemployment': 'UNRATE',\n",
    "        'Industrial_Production': 'INDPRO',\n",
    "        'Fed_Rate': 'FEDFUNDS',\n",
    "        'Consumer_Sentiment': 'UMCSENT'\n",
    "    }\n",
    "    \n",
    "    alignment_report = []  # Track what happened to each series\n",
    "    \n",
    "    for col_name, series_id in series_mapping.items():\n",
    "        print(f'  Processing {col_name} ({series_id})...')\n",
    "        \n",
    "        try:\n",
    "            # Get all vintage releases\n",
    "            vintages = fred.get_series_all_releases(series_id)\n",
    "            vintages['date'] = pd.to_datetime(vintages['date'])\n",
    "            vintages['realtime_start'] = pd.to_datetime(vintages['realtime_start'])\n",
    "            \n",
    "            # Create aligned series\n",
    "            aligned_series = pd.Series(index=df_aligned.index, dtype=float)\n",
    "            \n",
    "            for idx in df_aligned.index:\n",
    "                available_vintages = vintages[vintages['realtime_start'] <= idx]\n",
    "                \n",
    "                if len(available_vintages) > 0:\n",
    "                    past_obs = available_vintages[available_vintages['date'] < idx]\n",
    "                    \n",
    "                    if len(past_obs) > 0:\n",
    "                        most_recent = past_obs.sort_values(['date', 'realtime_start'], ascending=False).iloc[0]\n",
    "                        aligned_series.loc[idx] = most_recent['value']\n",
    "                    else:\n",
    "                        aligned_series.loc[idx] = np.nan\n",
    "                else:\n",
    "                    aligned_series.loc[idx] = np.nan\n",
    "            \n",
    "            # Count vintage coverage\n",
    "            vintage_count = aligned_series.notna().sum()\n",
    "            total_count = len(df_aligned)\n",
    "            \n",
    "            # Store original for comparison\n",
    "            original_series = aligned_series.copy()\n",
    "            \n",
    "            # If vintage has NaNs, fall back to original forward-filled values\n",
    "            aligned_series = aligned_series.fillna(df_aligned[col_name])\n",
    "            \n",
    "            # Count how many were filled vs vintage\n",
    "            filled_count = aligned_series.notna().sum() - vintage_count\n",
    "            still_missing = aligned_series.isna().sum()\n",
    "            \n",
    "            # Replace column\n",
    "            df_aligned[col_name] = aligned_series\n",
    "            \n",
    "            # Build report\n",
    "            first_valid = aligned_series.first_valid_index()\n",
    "            report = {\n",
    "                'series': col_name,\n",
    "                'vintage_values': vintage_count,\n",
    "                'forward_filled': filled_count,\n",
    "                'still_missing': still_missing,\n",
    "                'first_valid_date': first_valid\n",
    "            }\n",
    "            alignment_report.append(report)\n",
    "            \n",
    "            # Print summary for this series\n",
    "            if filled_count > 0 or still_missing > 0:\n",
    "                print(f'    ✓ Aligned {col_name}: {vintage_count} vintage, {filled_count} forward-filled, {still_missing} missing')\n",
    "                if first_valid:\n",
    "                    print(f'      First valid date: {first_valid.strftime(\"%Y-%m-%d\")}')\n",
    "            else:\n",
    "                print(f'    ✓ Aligned {col_name} ({vintage_count}/{total_count} all from vintage)')\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f'    Error aligning {col_name}: {str(e)}')\n",
    "            print(f'    Keeping original values')\n",
    "            alignment_report.append({\n",
    "                'series': col_name,\n",
    "                'vintage_values': 0,\n",
    "                'forward_filled': 0,\n",
    "                'still_missing': len(df_aligned),\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    # Print final summary\n",
    "    print('\\n VINTAGE ALIGNMENT SUMMARY:')\n",
    "    print('─' * 70)\n",
    "    for report in alignment_report:\n",
    "        if 'error' in report:\n",
    "            print(f\"  {report['series']}: ERROR - {report['error']}\")\n",
    "        else:\n",
    "            total = len(df_aligned)\n",
    "            vintage_pct = (report['vintage_values'] / total * 100) if total > 0 else 0\n",
    "            filled_pct = (report['forward_filled'] / total * 100) if total > 0 else 0\n",
    "            print(f\"  {report['series']}:\")\n",
    "            print(f\"    - Vintage dates: {report['vintage_values']:,} ({vintage_pct:.1f}%)\")\n",
    "            if report['forward_filled'] > 0:\n",
    "                print(f\"    - Forward-filled: {report['forward_filled']:,} ({filled_pct:.1f}%)\")\n",
    "            if report['still_missing'] > 0:\n",
    "                print(f\"    - Still missing: {report['still_missing']:,}\")\n",
    "    \n",
    "    return df_aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c1f19aa1-47e0-402b-ae80-f0c55c135c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_fixed_shift_alignment(df):\n",
    "    '''\n",
    "    apply fixed-day shift approximation for release dates.\n",
    "    '''\n",
    "    print_subsection('Applying fixed-shift release date alignment')\n",
    "    \n",
    "    # fix release date lags to prevent look-ahead bias\n",
    "    # shift forward = make data available LATER (i.e. when actually released)\n",
    "    df['CPI'] = df['CPI'].shift(14)  # released ~2 weeks after month end\n",
    "    # df['PCE'] = df['PCE'].shift(21)  # released ~3 weeks after month end\n",
    "    df['Unemployment'] = df['Unemployment'].shift(7)  # first fri of month\n",
    "    df['Industrial_Production'] = df['Industrial_Production'].shift(14)  # ~2 weeks\n",
    "    df['Fed_Rate'] = df['Fed_Rate'].shift(7)  # ~1 week after month end\n",
    "    df['Consumer_Sentiment'] = df['Consumer_Sentiment'].shift(2)  # final release ~month end    \n",
    "    # df['TB3MS'] = df['TB3MS'].shift(7)  # ~1 week after month end\n",
    "    print('   ✓ Release date adjustments applied')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5c523805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    print(\"\\nPreprocessing data...\")\n",
    "\n",
    "    # Print initial data quality\n",
    "    check_data_quality(df)\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"\\n=== CREDIT SPREAD MISSING DATA DIAGNOSTICS ===\")\n",
    "    if 'Credit_HY' in df.columns:\n",
    "        missing_hy = df['Credit_HY'].isnull()\n",
    "        missing_ig = df['Credit_IG'].isnull()\n",
    "        \n",
    "        print(f\"Credit_HY missing: {missing_hy.sum()}\")\n",
    "        print(f\"Credit_IG missing: {missing_ig.sum()}\")\n",
    "        \n",
    "        if missing_hy.sum() > 0:\n",
    "            print(f\"First missing date: {df[missing_hy].index.min()}\")\n",
    "            print(f\"Last missing date: {df[missing_hy].index.max()}\")\n",
    "            print(f\"First valid date: {df[~missing_hy].index.min()}\")\n",
    "            \n",
    "        # Check if missing values are contiguous (all at start/end)\n",
    "        first_valid_idx = df['Credit_HY'].first_valid_index()\n",
    "        last_valid_idx = df['Credit_HY'].last_valid_index()\n",
    "        print(f\"Credit spreads valid from {first_valid_idx} to {last_valid_idx}\")\n",
    "\n",
    "    print(\"\\n=== MISSING DATA PATTERN ===\")\n",
    "    # Check how missing values are distributed\n",
    "    missing_dates = df[df['Credit_HY'].isnull()].index\n",
    "    missing_by_year = missing_dates.year.value_counts().sort_index()\n",
    "    print(\"Missing Credit_HY observations by year:\")\n",
    "    print(missing_by_year)\n",
    "    \n",
    "    # Sample of dates with missing credit spreads\n",
    "    print(\"\\nSample of dates with missing credit spreads (first 20):\")\n",
    "    print(missing_dates[:20].tolist())\n",
    "    \n",
    "    # Check day of week pattern\n",
    "    print(\"\\nMissing by day of week:\")\n",
    "    print(missing_dates.dayofweek.value_counts().sort_index())\n",
    "    print(\"(0=Monday, 6=Sunday)\")\n",
    "    \"\"\"\n",
    "    \n",
    "    # forward-fill macro variables (monthly data in daily frequency)\n",
    "    # market data (VIX, SP500, yields) should not have missing values\n",
    "    print_subsection('Forward-filling macro variables...')\n",
    "    macro_vars = ['CPI', 'Unemployment', 'Fed_Rate', \n",
    "                  'Consumer_Sentiment', 'Industrial_Production',\n",
    "                   ]\n",
    "    df_clean = df.copy()\n",
    "    df_clean[macro_vars] = df_clean[macro_vars].fillna(method='ffill')\n",
    "    \n",
    "    # verify no remaining NaNs in critical columns\n",
    "    remaining_na = df_clean[macro_vars].isnull().sum()\n",
    "    if remaining_na.sum() > 0:\n",
    "        print('   Warning: NaN values remain after forward-fill:')\n",
    "        print(remaining_na[remaining_na > 0])\n",
    "    else:\n",
    "        print('   ✓ All macro variables forward-filled successfully')\n",
    "\n",
    "    # apply release date alignment based on configuration\n",
    "    if USE_VINTAGE_DATES:\n",
    "        df_clean = apply_vintage_date_alignment(df_clean, FRED_API_KEY)\n",
    "    else:\n",
    "        df_clean = apply_fixed_shift_alignment(df_clean)\n",
    "    \n",
    "    # shifts will create NaNs that need to be removed\n",
    "    df_clean = df_clean.dropna()\n",
    "\n",
    "    if 'CPI' in df_clean.columns:\n",
    "        df_clean['Inflation_YoY'] = df_clean['CPI'].pct_change(252) * 100  # 252 trading days ≈ 1 year\n",
    "\n",
    "    # if 'PCE' in df_clean.columns:\n",
    "    #     df_clean['PCE_YoY'] = df_clean['PCE'].pct_change(12) * 100\n",
    "\n",
    "    if 'SP500_Close' in df_clean.columns:\n",
    "        df_clean['SP500_Volatility'] = df_clean['SP500_Returns'].rolling(20).std()\n",
    "\n",
    "    print(f\"   ✓ Final dataset shape: {df_clean.shape}\")\n",
    "    print(f\"   ✓ Date range: {df_clean.index[0]} to {df_clean.index[-1]}\")\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c70702fc-359f-4fe1-b1d5-76ea539a0571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_to_frequency(df, freq='ME'):\n",
    "    '''\n",
    "    Resample daily data to weekly or monthly frequency.\n",
    "\n",
    "    - takes last value for price levels\n",
    "    - recalculates returns from resampled prices\n",
    "    - for monthly: applies 1-month lag to macro variables\n",
    "    '''\n",
    "    print_subsection(f'\\nResampling to {freq} frequency...')\n",
    "    \n",
    "    # Define aggregation rules\n",
    "    # Special case: volume gets averaged, everything else gets end-of-period\n",
    "    agg_rules = {}\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if 'Volume' in col:\n",
    "            agg_rules[col] = 'mean'  # Average daily volume\n",
    "        else:\n",
    "            agg_rules[col] = 'last'  # End-of-period value for prices, rates, indices\n",
    "    \n",
    "    # Resample\n",
    "    resampled = df.resample(freq).agg(agg_rules)\n",
    "    \n",
    "    # Recalculate derived features from resampled data\n",
    "    if 'SP500_Close' in resampled.columns:\n",
    "        resampled['SP500_Returns'] = resampled['SP500_Close'].pct_change() * 100\n",
    "    if 'CPI' in resampled.columns:\n",
    "        resampled['Inflation_YoY'] = resampled['CPI'].pct_change(12) * 100\n",
    "    if 'SP500_Returns' in resampled.columns:\n",
    "        resampled['SP500_Volatility'] = resampled['SP500_Returns'].rolling(12).std()\n",
    "    \n",
    "    # For monthly frequency: apply additional 1-period lag to macro variables\n",
    "    if freq in ['M', 'ME', 'MS']:  # Month end, month start\n",
    "        macro_vars = ['CPI', 'Unemployment', 'Industrial_Production', \n",
    "                      'Fed_Rate', 'Consumer_Sentiment']\n",
    "        for var in macro_vars:\n",
    "            if var in resampled.columns:\n",
    "                resampled[var] = resampled[var].shift(1)\n",
    "    \n",
    "    # Drop NaNs from rolling windows and shifts\n",
    "    resampled = resampled.dropna()\n",
    "    \n",
    "    print(f'   ✓ Resampled shape: {resampled.shape}')\n",
    "    print(f'   ✓ Date range: {resampled.index[0]} to {resampled.index[-1]}')\n",
    "    return resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aed88dea-4771-4d85-b104-66d076c2e734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_val_test_split(df, train_pct = 0.7, val_pct = 0.15):\n",
    "    '''\n",
    "    create temporal train/validation/test splits\n",
    "    \n",
    "    params:\n",
    "    -------\n",
    "    df : dataframe w/ DatetimeIndex\n",
    "    train_pct : float, proportion for training (default 0.7)\n",
    "    val_pct : float, proportion for validation (default 0.15)\n",
    "    \n",
    "    return:\n",
    "    -------\n",
    "    train, val, test : tuple of dataframes\n",
    "    \n",
    "    notes:\n",
    "    -------\n",
    "    uses temporal splits (not random) to avoid lookahead\n",
    "    test set is most recent data\n",
    "    '''\n",
    "    n = len(df)\n",
    "    train_end = int(n * train_pct)\n",
    "    val_end = int(n * (train_pct + val_pct))\n",
    "    \n",
    "    train = df.iloc[:train_end].copy()\n",
    "    val = df.iloc[train_end:val_end].copy()\n",
    "    test = df.iloc[val_end:].copy()\n",
    "    \n",
    "    print('\\n DATA SPLITS:')\n",
    "    print('─' * 70)\n",
    "    print(f'Train: {train.index[0].strftime(\"%Y-%m-%d\")} to {train.index[-1].strftime(\"%Y-%m-%d\")} ({len(train):,} obs, {train_pct*100:.0f}%)')\n",
    "    print(f'Val:   {val.index[0].strftime(\"%Y-%m-%d\")} to {val.index[-1].strftime(\"%Y-%m-%d\")} ({len(val):,} obs, {val_pct*100:.0f}%)')\n",
    "    print(f'Test:  {test.index[0].strftime(\"%Y-%m-%d\")} to {test.index[-1].strftime(\"%Y-%m-%d\")} ({len(test):,} obs, {(1-train_pct-val_pct)*100:.0f}%)')\n",
    "    \n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4d8a6d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(df, filename='financial_dataset.csv', label=''):\n",
    "    \"\"\"Save the processed dataset\"\"\"\n",
    "    df.to_csv(filename)\n",
    "\n",
    "    header = f' FINAL DATA SUMMARY - {label}' if label else ' FINAL DATA SUMMARY'\n",
    "    print('\\n' + '='*70)\n",
    "    print(header)\n",
    "    print('='*70)\n",
    "    print(f'\\nShape: {df.shape}')\n",
    "    print(f'Date range: {df.index[0]} to {df.index[-1]}')\n",
    "    print(f'Years covered: {(df.index[-1] - df.index[0]).days / 365.25:.1f}')\n",
    "    print(f'\\nColumns: {list(df.columns)}')\n",
    "    print(f'\\nMemory usage: {df.memory_usage(deep=True).sum() / 1024:.1f} KB')\n",
    "    print(f'\\nTarget variable (SP500_Returns) statistics:')\n",
    "    print(f'  Mean: {df[\"SP500_Returns\"].mean():.4f}%')\n",
    "    print(f'  Std: {df[\"SP500_Returns\"].std():.4f}%')\n",
    "    print(f'  Min: {df[\"SP500_Returns\"].min():.4f}%')\n",
    "    print(f'  Max: {df[\"SP500_Returns\"].max():.4f}%')\n",
    "    print(f'  Skewness: {df[\"SP500_Returns\"].skew():.4f}')\n",
    "    print(f'  Kurtosis: {df[\"SP500_Returns\"].kurtosis():.4f}')\n",
    "    print(f'\\nMissing values: {df.isnull().sum().sum()}')\n",
    "\n",
    "    if df.isnull().sum().sum() > 0:\n",
    "        print('\\nWARNING - Missing values by column:')\n",
    "        for col in df.columns:\n",
    "            missing = df[col].isnull().sum()\n",
    "            if missing > 0:\n",
    "                print(f'  {col}: {missing}')\n",
    "\n",
    "    print(f\"\\nSaved dataset to '{filename}'\")\n",
    "    \n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b8eb317a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FINANCIAL DATA COLLECTION PIPELINE\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Collecting S&P 500 Market Data\n",
      "----------------------------------------------------------------------\n",
      "   Collected 9007 days of S&P 500 data\n",
      "   Date range: 1990-01-02 00:00:00 to 2025-10-06 00:00:00\n",
      "\n",
      "Collecting FRED Macroeconomic Data\n",
      "----------------------------------------------------------------------\n",
      "   ✓ VIX: 9329 observations\n",
      "   ✓ Treasury_10Y: 16634 observations\n",
      "   ✓ Yield_Spread: 12875 observations\n",
      "   ✓ CPI: 944 observations\n",
      "   ✓ Unemployment: 932 observations\n",
      "   ✓ Fed_Rate: 855 observations\n",
      "   ✓ Consumer_Sentiment: 874 observations\n",
      "   ✓ Industrial_Production: 1280 observations\n",
      "\n",
      "   Summary: Successfully collected 8/8 series\n",
      "\n",
      "Collecting Yahoo Finance Data\n",
      "----------------------------------------------------------------------\n",
      "   ✓ Wilshire5000 (^W5000): 8999 observations\n",
      "\n",
      "   Summary: Successfully collected 1/1 tickers\n",
      "\n",
      "Aligning and combining data...\n",
      "\n",
      "Preprocessing data...\n",
      "\n",
      "Data Quality Check\n",
      "----------------------------------------------------------------------\n",
      "Total missing values: 150\n",
      "\n",
      "Missing by column:\n",
      "  SP500_Returns: 1 (0.0%)\n",
      "  VIX: 3 (0.0%)\n",
      "  Treasury_10Y: 73 (0.8%)\n",
      "  Yield_Spread: 73 (0.8%)\n",
      "\n",
      "Extreme daily returns (>10%): 3\n",
      "Dates with extreme returns:\n",
      "  2008-10-13: 11.58%\n",
      "  2008-10-28: 10.79%\n",
      "  2020-03-16: -11.98%\n",
      "\n",
      "Data types:\n",
      "SP500_Close              float64\n",
      "SP500_Volume               int64\n",
      "SP500_Returns            float64\n",
      "VIX                      float64\n",
      "Treasury_10Y             float64\n",
      "Yield_Spread             float64\n",
      "CPI                      float64\n",
      "Unemployment             float64\n",
      "Fed_Rate                 float64\n",
      "Consumer_Sentiment       float64\n",
      "Industrial_Production    float64\n",
      "Wilshire5000             float64\n",
      "dtype: object\n",
      "\n",
      "Forward-filling macro variables...\n",
      "----------------------------------------------------------------------\n",
      "   ✓ All macro variables forward-filled successfully\n",
      "\n",
      "Applying fixed-shift release date alignment\n",
      "----------------------------------------------------------------------\n",
      "   ✓ Release date adjustments applied\n",
      "   ✓ Final dataset shape: (8918, 14)\n",
      "   ✓ Date range: 1990-01-22 00:00:00 to 2025-10-06 00:00:00\n",
      "\n",
      "======================================================================\n",
      " FINAL DATA SUMMARY - DAILY\n",
      "======================================================================\n",
      "\n",
      "Shape: (8918, 14)\n",
      "Date range: 1990-01-22 00:00:00 to 2025-10-06 00:00:00\n",
      "Years covered: 35.7\n",
      "\n",
      "Columns: ['SP500_Close', 'SP500_Volume', 'SP500_Returns', 'VIX', 'Treasury_10Y', 'Yield_Spread', 'CPI', 'Unemployment', 'Fed_Rate', 'Consumer_Sentiment', 'Industrial_Production', 'Wilshire5000', 'Inflation_YoY', 'SP500_Volatility']\n",
      "\n",
      "Memory usage: 1045.1 KB\n",
      "\n",
      "Target variable (SP500_Returns) statistics:\n",
      "  Mean: 0.0378%\n",
      "  Std: 1.1345%\n",
      "  Min: -11.9841%\n",
      "  Max: 10.7890%\n",
      "  Skewness: -0.2655\n",
      "  Kurtosis: 9.8354\n",
      "\n",
      "Missing values: 271\n",
      "\n",
      "WARNING - Missing values by column:\n",
      "  Inflation_YoY: 252\n",
      "  SP500_Volatility: 19\n",
      "\n",
      "Saved dataset to 'financial_dataset_daily.csv'\n",
      "\n",
      "\n",
      "Resampling to ME frequency...\n",
      "----------------------------------------------------------------------\n",
      "   ✓ Resampled shape: (418, 14)\n",
      "   ✓ Date range: 1991-01-31 00:00:00 to 2025-10-31 00:00:00\n",
      "\n",
      "======================================================================\n",
      " FINAL DATA SUMMARY - MONTHLY\n",
      "======================================================================\n",
      "\n",
      "Shape: (418, 14)\n",
      "Date range: 1991-01-31 00:00:00 to 2025-10-31 00:00:00\n",
      "Years covered: 34.7\n",
      "\n",
      "Columns: ['SP500_Close', 'SP500_Volume', 'SP500_Returns', 'VIX', 'Treasury_10Y', 'Yield_Spread', 'CPI', 'Unemployment', 'Fed_Rate', 'Consumer_Sentiment', 'Industrial_Production', 'Wilshire5000', 'Inflation_YoY', 'SP500_Volatility']\n",
      "\n",
      "Memory usage: 49.0 KB\n",
      "\n",
      "Target variable (SP500_Returns) statistics:\n",
      "  Mean: 0.8143%\n",
      "  Std: 4.2270%\n",
      "  Min: -16.9425%\n",
      "  Max: 12.6844%\n",
      "  Skewness: -0.5716\n",
      "  Kurtosis: 1.1004\n",
      "\n",
      "Missing values: 0\n",
      "\n",
      "Saved dataset to 'financial_dataset_monthly.csv'\n",
      "\n",
      "Data processing done and saved.\n"
     ]
    }
   ],
   "source": [
    "print_section('FINANCIAL DATA COLLECTION PIPELINE')\n",
    "\n",
    "try:\n",
    "    # Collect all data\n",
    "    market_data = collect_market_data()\n",
    "    fred_data = collect_fred_data(FRED_API_KEY)\n",
    "    yahoo_data = collect_yahoo_data(YAHOO_TICKERS)\n",
    "\n",
    "    # Combine and process\n",
    "    combined_data = align_and_combine_data(market_data, fred_data, yahoo_data)\n",
    "    final_data = preprocess_data(combined_data)\n",
    "\n",
    "    # save daily version (raw)\n",
    "    daily_filename = save_data(final_data, 'financial_dataset_daily.csv', 'DAILY')\n",
    "    \n",
    "    # create + save monthly version\n",
    "    monthly_data = resample_to_frequency(final_data, freq='ME')\n",
    "    monthly_filename = save_data(monthly_data, 'financial_dataset_monthly.csv', 'MONTHLY')\n",
    "\n",
    "    print(\"\\nData processing done and saved.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nError during data collection: {str(e)}\")\n",
    "    print(\"Please check your FRED API key and internet connection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "763018ad-e036-473d-b583-945dce59fba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DATA COLLECTION COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Generated files:\n",
      "  - financial_dataset_daily.csv\n",
      "  - financial_dataset_monthly.csv\n",
      "\n",
      "Next steps:\n",
      "  1. See feature_configs.py for predefined feature sets\n",
      "  2. Use data_utils.py to load feature sets for modeling\n",
      "  3. Example: data = load_feature_set('core_proposal', frequency='monthly')\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA COLLECTION COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"  - financial_dataset_daily.csv\")\n",
    "print(\"  - financial_dataset_monthly.csv\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  1. See feature_configs.py for predefined feature sets\")\n",
    "print(\"  2. Use data_utils.py to load feature sets for modeling\")\n",
    "print(\"  3. Example: data = load_feature_set('core_proposal', frequency='monthly')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f03bb5-3af7-4b25-83a2-963c71d36e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n Creating visualizations...')\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
    "\n",
    "# 1. returns over time\n",
    "axes[0].plot(final_data.index, final_data['SP500_Returns'], alpha=0.7, linewidth=0.5)\n",
    "axes[0].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "axes[0].set_title('S&P 500 Daily Returns Over Time', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Return (%)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. distribution\n",
    "axes[1].hist(final_data['SP500_Returns'].dropna(), bins=100, \n",
    "             edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(x=0, color='red', linestyle='--', alpha=0.5)\n",
    "axes[1].set_title('Distribution of Daily Returns', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Return (%)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. cumulative returns\n",
    "cumulative = (1 + final_data['SP500_Returns']/100).cumprod()\n",
    "axes[2].plot(final_data.index, cumulative, linewidth=1.5)\n",
    "axes[2].set_title('Cumulative Returns (Growth of $1)', fontsize=14, fontweight='bold')\n",
    "axes[2].set_ylabel('Value ($)')\n",
    "axes[2].set_xlabel('Date')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('data_overview.png', dpi=150, bbox_inches='tight')\n",
    "print('   Saved visualization: data_overview.png')\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "598-dl-final",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
