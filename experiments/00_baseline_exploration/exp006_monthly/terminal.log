
[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
Global seed set to 42
======================================================================
Training TFT: exp006_monthly
======================================================================
Configuration saved to: experiments/exp006_monthly/config.json

Loading data splits...
Train: 292 samples
Val: 63 samples
Test: 63 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 4

Initializing model...
Model parameters: 66,975
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs

Starting training...
Checkpoints will be saved to: experiments/exp006_monthly/checkpoints/
Logs will be saved to: experiments/exp006_monthly/logs/

You are using a CUDA device ('NVIDIA GeForce RTX 3070') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 1.8 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 9.8 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.8 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 4.3 K 
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 4.3 K 
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 4.3 K 
10 | static_context_enrichment          | GatedResidualNetwork            | 4.3 K 
11 | lstm_encoder                       | LSTM                            | 8.4 K 
12 | lstm_decoder                       | LSTM                            | 8.4 K 
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.1 K 
14 | post_lstm_add_norm_encoder         | AddNorm                         | 64    
15 | static_enrichment                  | GatedResidualNetwork            | 5.3 K 
16 | multihead_attn                     | InterpretableMultiHeadAttention | 3.2 K 
17 | post_attn_gate_norm                | GateAddNorm                     | 2.2 K 
18 | pos_wise_ff                        | GatedResidualNetwork            | 4.3 K 
19 | pre_output_gate_norm               | GateAddNorm                     | 2.2 K 
20 | output_layer                       | Linear                          | 231   
----------------------------------------------------------------------------------------
67.0 K    Trainable params
0         Non-trainable params
67.0 K    Total params
0.268     Total estimated model params size (MB)
Epoch 0: val_loss=N/A
Epoch 0: val_loss=4.9092936515808105
Epoch 0: train_loss=2.9987025260925293
Epoch 1: val_loss=4.885292053222656
Epoch 1: train_loss=2.971620559692383
Epoch 2: val_loss=4.863885879516602
Epoch 2: train_loss=2.917696475982666
Epoch 3: val_loss=4.777838230133057
Epoch 3: train_loss=2.923487663269043
Epoch 4: val_loss=4.739772796630859
Epoch 4: train_loss=2.893834114074707
Epoch 5: val_loss=4.706167697906494
Epoch 5: train_loss=2.8629400730133057
Epoch 6: val_loss=4.565380096435547
Epoch 6: train_loss=2.827730894088745
Epoch 7: val_loss=4.519399166107178
Epoch 7: train_loss=2.7203621864318848
Epoch 8: val_loss=4.477109909057617
Epoch 8: train_loss=2.679088830947876
Epoch 9: val_loss=4.303863525390625
Epoch 9: train_loss=2.656287431716919
Epoch 10: val_loss=4.251414775848389
Epoch 10: train_loss=2.5355100631713867
Epoch 11: val_loss=4.200329780578613
Epoch 11: train_loss=2.5445539951324463
Epoch 12: val_loss=3.995816946029663
Epoch 12: train_loss=2.4624240398406982
Epoch 13: val_loss=3.9407424926757812
Epoch 13: train_loss=2.475309133529663
Epoch 14: val_loss=3.885830879211426
Epoch 14: train_loss=2.3601295948028564
Epoch 15: val_loss=3.6770613193511963
Epoch 15: train_loss=2.406906843185425
Epoch 16: val_loss=3.619424343109131
Epoch 16: train_loss=2.2539074420928955
Epoch 17: val_loss=3.5653698444366455
Epoch 17: train_loss=2.2083740234375
Epoch 18: val_loss=3.353325366973877
Epoch 18: train_loss=2.185335159301758
Epoch 19: val_loss=3.306022882461548
Epoch 19: train_loss=2.1500163078308105
Epoch 20: val_loss=3.2524733543395996
Epoch 20: train_loss=2.0508053302764893
Epoch 21: val_loss=3.0581517219543457
Epoch 21: train_loss=2.057215929031372
Epoch 22: val_loss=3.0084567070007324
Epoch 22: train_loss=1.9828600883483887
Epoch 23: val_loss=2.9607529640197754
Epoch 23: train_loss=1.9951786994934082
Epoch 24: val_loss=2.7744479179382324
Epoch 24: train_loss=1.9240013360977173
Epoch 25: val_loss=2.7232303619384766
Epoch 25: train_loss=1.9283756017684937
Epoch 26: val_loss=2.6709842681884766
Epoch 26: train_loss=1.9358941316604614
Epoch 27: val_loss=2.4704158306121826
Epoch 27: train_loss=1.863638997077942
Epoch 28: val_loss=2.4252266883850098
Epoch 28: train_loss=1.8554871082305908
Epoch 29: val_loss=2.3794727325439453
Epoch 29: train_loss=1.8459820747375488
Epoch 30: val_loss=2.306795120239258
Epoch 30: train_loss=1.8171740770339966
Epoch 31: val_loss=2.2863476276397705
Epoch 31: train_loss=1.7271993160247803
Epoch 32: val_loss=2.2698609828948975
Epoch 32: train_loss=1.8130648136138916
Epoch 33: val_loss=2.2124156951904297
Epoch 33: train_loss=1.7566757202148438
Epoch 34: val_loss=2.212533950805664
Epoch 34: train_loss=1.8073151111602783
Epoch 35: val_loss=2.1919755935668945
Epoch 35: train_loss=1.7646758556365967
Epoch 36: val_loss=2.1342058181762695
Epoch 36: train_loss=1.7479276657104492
Epoch 37: val_loss=2.1249258518218994
Epoch 37: train_loss=1.766425371170044
Epoch 38: val_loss=2.11118221282959
Epoch 38: train_loss=1.7222130298614502
Epoch 39: val_loss=2.0566608905792236
Epoch 39: train_loss=1.7448539733886719
Epoch 40: val_loss=2.046193838119507
Epoch 40: train_loss=1.7632454633712769
Epoch 41: val_loss=2.0336835384368896
Epoch 41: train_loss=1.7263033390045166
Epoch 42: val_loss=1.9995685815811157
Epoch 42: train_loss=1.7148449420928955
Epoch 43: val_loss=1.9879616498947144
Epoch 43: train_loss=1.7432013750076294
Epoch 44: val_loss=1.96807062625885
Epoch 44: train_loss=1.7298688888549805
Epoch 45: val_loss=1.9265350103378296
Epoch 45: train_loss=1.7386250495910645
Epoch 46: val_loss=1.9285714626312256
Epoch 46: train_loss=1.720625400543213
Epoch 47: val_loss=1.9233180284500122
Epoch 47: train_loss=1.7588350772857666
Epoch 48: val_loss=1.910372018814087
Epoch 48: train_loss=1.760994791984558
Epoch 49: val_loss=1.9109731912612915
Epoch 49: train_loss=1.7418040037155151
`Trainer.fit` stopped: `max_epochs=50` reached.


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/exp006_monthly/checkpoints/tft-epoch=48-val_loss=1.9104.ckpt
Best validation loss: 1.910372
