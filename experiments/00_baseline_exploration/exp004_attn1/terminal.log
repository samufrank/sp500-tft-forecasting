
[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
Global seed set to 42
======================================================================
Training TFT: exp004_attn1
======================================================================
Configuration saved to: experiments/exp004_attn1/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 68,015
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs

Starting training...
Checkpoints will be saved to: experiments/exp004_attn1/checkpoints/
Logs will be saved to: experiments/exp004_attn1/logs/

You are using a CUDA device ('NVIDIA GeForce RTX 3070') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 1.8 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 9.8 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.8 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 4.3 K 
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 4.3 K 
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 4.3 K 
10 | static_context_enrichment          | GatedResidualNetwork            | 4.3 K 
11 | lstm_encoder                       | LSTM                            | 8.4 K 
12 | lstm_decoder                       | LSTM                            | 8.4 K 
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.1 K 
14 | post_lstm_add_norm_encoder         | AddNorm                         | 64    
15 | static_enrichment                  | GatedResidualNetwork            | 5.3 K 
16 | multihead_attn                     | InterpretableMultiHeadAttention | 4.2 K 
17 | post_attn_gate_norm                | GateAddNorm                     | 2.2 K 
18 | pos_wise_ff                        | GatedResidualNetwork            | 4.3 K 
19 | pre_output_gate_norm               | GateAddNorm                     | 2.2 K 
20 | output_layer                       | Linear                          | 231   
----------------------------------------------------------------------------------------
68.0 K    Trainable params
0         Non-trainable params
68.0 K    Total params
0.272     Total estimated model params size (MB)
Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.11910074949264526
Epoch 0: train_loss=0.7467504739761353
Epoch 1: val_loss=0.16690194606781006
Epoch 1: train_loss=0.4891779124736786
Epoch 2: val_loss=0.18735897541046143
Epoch 2: train_loss=0.454703152179718
Epoch 3: val_loss=0.2036314606666565
Epoch 3: train_loss=0.4455188512802124
Epoch 4: val_loss=0.21358786523342133
Epoch 4: train_loss=0.43858110904693604
Epoch 5: val_loss=0.2243126779794693
Epoch 5: train_loss=0.4342469871044159
Epoch 6: val_loss=0.22343677282333374
Epoch 6: train_loss=0.4326857030391693
Epoch 7: val_loss=0.20629894733428955
Epoch 7: train_loss=0.4316042363643646
Epoch 8: val_loss=0.21695874631404877
Epoch 8: train_loss=0.43061208724975586
Epoch 9: val_loss=0.2105337381362915
Epoch 9: train_loss=0.4287545084953308
Epoch 10: val_loss=0.2181292325258255
Epoch 10: train_loss=0.42803335189819336


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/exp004_attn1/checkpoints/tft-epoch=00-val_loss=0.1191.ckpt
Best validation loss: 0.119101
