
[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
Global seed set to 42
======================================================================
Training TFT: exp005_attn4
======================================================================
Configuration saved to: experiments/exp005_attn4/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 66,455
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs

Starting training...
Checkpoints will be saved to: experiments/exp005_attn4/checkpoints/
Logs will be saved to: experiments/exp005_attn4/logs/

You are using a CUDA device ('NVIDIA GeForce RTX 3070') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 1.8 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 9.8 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.8 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 4.3 K 
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 4.3 K 
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 4.3 K 
10 | static_context_enrichment          | GatedResidualNetwork            | 4.3 K 
11 | lstm_encoder                       | LSTM                            | 8.4 K 
12 | lstm_decoder                       | LSTM                            | 8.4 K 
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.1 K 
14 | post_lstm_add_norm_encoder         | AddNorm                         | 64    
15 | static_enrichment                  | GatedResidualNetwork            | 5.3 K 
16 | multihead_attn                     | InterpretableMultiHeadAttention | 2.6 K 
17 | post_attn_gate_norm                | GateAddNorm                     | 2.2 K 
18 | pos_wise_ff                        | GatedResidualNetwork            | 4.3 K 
19 | pre_output_gate_norm               | GateAddNorm                     | 2.2 K 
20 | output_layer                       | Linear                          | 231   
----------------------------------------------------------------------------------------
66.5 K    Trainable params
0         Non-trainable params
66.5 K    Total params
0.266     Total estimated model params size (MB)
Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.1939094066619873
Epoch 0: train_loss=0.5879656672477722
Epoch 1: val_loss=0.17683684825897217
Epoch 1: train_loss=0.4686799645423889
Epoch 2: val_loss=0.18156218528747559
Epoch 2: train_loss=0.45174890756607056
Epoch 3: val_loss=0.22566819190979004
Epoch 3: train_loss=0.44034355878829956
Epoch 4: val_loss=0.22341305017471313
Epoch 4: train_loss=0.43673887848854065
Epoch 5: val_loss=0.2541707158088684
Epoch 5: train_loss=0.4332154393196106
Epoch 6: val_loss=0.26662153005599976
Epoch 6: train_loss=0.43029123544692993
Epoch 7: val_loss=0.21763429045677185
Epoch 7: train_loss=0.42890965938568115
Epoch 8: val_loss=0.23629102110862732
Epoch 8: train_loss=0.4282434284687042
Epoch 9: val_loss=0.2091480791568756
Epoch 9: train_loss=0.42667320370674133
Epoch 10: val_loss=0.19698549807071686
Epoch 10: train_loss=0.42513030767440796
Epoch 11: val_loss=0.21633106470108032
Epoch 11: train_loss=0.4260197579860687


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/exp005_attn4/checkpoints/tft-epoch=01-val_loss=0.1768.ckpt
Best validation loss: 0.176837
