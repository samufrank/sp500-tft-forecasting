
[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
Global seed set to 42
======================================================================
Training TFT: exp002_small
======================================================================
Configuration saved to: experiments/exp002_small/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 22,615
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs

Starting training...
Checkpoints will be saved to: experiments/exp002_small/checkpoints/
Logs will be saved to: experiments/exp002_small/logs/

You are using a CUDA device ('NVIDIA GeForce RTX 3070') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 1.2 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 6.4 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.2 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K 
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K 
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K 
10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K 
11 | lstm_encoder                       | LSTM                            | 2.2 K 
12 | lstm_decoder                       | LSTM                            | 2.2 K 
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   
14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    
15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K 
16 | multihead_attn                     | InterpretableMultiHeadAttention | 808   
17 | post_attn_gate_norm                | GateAddNorm                     | 576   
18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K 
19 | pre_output_gate_norm               | GateAddNorm                     | 576   
20 | output_layer                       | Linear                          | 119   
----------------------------------------------------------------------------------------
22.6 K    Trainable params
0         Non-trainable params
22.6 K    Total params
0.090     Total estimated model params size (MB)
Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.21046677231788635
Epoch 0: train_loss=0.824827253818512
Epoch 1: val_loss=0.15519025921821594
Epoch 1: train_loss=0.5342864394187927
Epoch 2: val_loss=0.1884336918592453
Epoch 2: train_loss=0.4620942175388336
Epoch 3: val_loss=0.19631317257881165
Epoch 3: train_loss=0.45649322867393494
Epoch 4: val_loss=0.20542922616004944
Epoch 4: train_loss=0.4516260623931885
Epoch 5: val_loss=0.21925915777683258
Epoch 5: train_loss=0.4467702805995941
Epoch 6: val_loss=0.2392427921295166
Epoch 6: train_loss=0.4434363543987274
Epoch 7: val_loss=0.2409256100654602
Epoch 7: train_loss=0.43897414207458496
Epoch 8: val_loss=0.2555397152900696
Epoch 8: train_loss=0.43925657868385315
Epoch 9: val_loss=0.2638099491596222
Epoch 9: train_loss=0.4370840787887573
Epoch 10: val_loss=0.2678390145301819
Epoch 10: train_loss=0.4355161488056183
Epoch 11: val_loss=0.27249234914779663
Epoch 11: train_loss=0.4337611198425293


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/exp002_small/checkpoints/tft-epoch=01-val_loss=0.1552.ckpt
Best validation loss: 0.155190
