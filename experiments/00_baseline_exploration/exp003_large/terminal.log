
[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
Global seed set to 42
======================================================================
Training TFT: exp003_large
======================================================================
Configuration saved to: experiments/exp003_large/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 231,823
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs

Starting training...
Checkpoints will be saved to: experiments/exp003_large/checkpoints/
Logs will be saved to: experiments/exp003_large/logs/

You are using a CUDA device ('NVIDIA GeForce RTX 3070') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 3.1 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 16.2 K
6  | decoder_variable_selection         | VariableSelectionNetwork        | 3.1 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 16.8 K
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 16.8 K
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 16.8 K
10 | static_context_enrichment          | GatedResidualNetwork            | 16.8 K
11 | lstm_encoder                       | LSTM                            | 33.3 K
12 | lstm_decoder                       | LSTM                            | 33.3 K
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 8.3 K 
14 | post_lstm_add_norm_encoder         | AddNorm                         | 128   
15 | static_enrichment                  | GatedResidualNetwork            | 20.9 K
16 | multihead_attn                     | InterpretableMultiHeadAttention | 12.4 K
17 | post_attn_gate_norm                | GateAddNorm                     | 8.4 K 
18 | pos_wise_ff                        | GatedResidualNetwork            | 16.8 K
19 | pre_output_gate_norm               | GateAddNorm                     | 8.4 K 
20 | output_layer                       | Linear                          | 455   
----------------------------------------------------------------------------------------
231 K     Trainable params
0         Non-trainable params
231 K     Total params
0.927     Total estimated model params size (MB)
Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.14767727255821228
Epoch 0: train_loss=0.7956357002258301
Epoch 1: val_loss=0.22067353129386902
Epoch 1: train_loss=0.45883676409721375
Epoch 2: val_loss=0.2812255024909973
Epoch 2: train_loss=0.43994396924972534
Epoch 3: val_loss=0.26605933904647827
Epoch 3: train_loss=0.4331240952014923
Epoch 4: val_loss=0.30401742458343506
Epoch 4: train_loss=0.43169382214546204
Epoch 5: val_loss=0.3446601927280426
Epoch 5: train_loss=0.4300050139427185
Epoch 6: val_loss=0.3231458067893982
Epoch 6: train_loss=0.42733991146087646
Epoch 7: val_loss=0.31820040941238403
Epoch 7: train_loss=0.4244955778121948
Epoch 8: val_loss=0.34316158294677734
Epoch 8: train_loss=0.4253736734390259
Epoch 9: val_loss=0.3282272219657898
Epoch 9: train_loss=0.4249396324157715
Epoch 10: val_loss=0.2860565483570099
Epoch 10: train_loss=0.4257737696170807


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/exp003_large/checkpoints/tft-epoch=00-val_loss=0.1477.ckpt
Best validation loss: 0.147677
