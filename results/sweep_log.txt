nohup: ignoring input
Starting hyperparameter sweep at Thu Oct 16 19:22:23 MST 2025
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 1.8 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 9.8 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.8 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 4.3 K 
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 4.3 K 
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 4.3 K 
10 | static_context_enrichment          | GatedResidualNetwork            | 4.3 K 
11 | lstm_encoder                       | LSTM                            | 8.4 K 
12 | lstm_decoder                       | LSTM                            | 8.4 K 
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.1 K 
14 | post_lstm_add_norm_encoder         | AddNorm                         | 64    
15 | static_enrichment                  | GatedResidualNetwork            | 5.3 K 
16 | multihead_attn                     | InterpretableMultiHeadAttention | 3.2 K 
17 | post_attn_gate_norm                | GateAddNorm                     | 2.2 K 
18 | pos_wise_ff                        | GatedResidualNetwork            | 4.3 K 
19 | pre_output_gate_norm               | GateAddNorm                     | 2.2 K 
20 | output_layer                       | Linear                          | 231   
----------------------------------------------------------------------------------------
67.0 K    Trainable params
0         Non-trainable params
67.0 K    Total params
0.268     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep_baseline
======================================================================
Configuration saved to: experiments/sweep_baseline/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 66,975

Starting training...
Checkpoints will be saved to: experiments/sweep_baseline/checkpoints/
Logs will be saved to: experiments/sweep_baseline/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.13702630996704102
Epoch 0: train_loss=0.6329951286315918
Epoch 1: val_loss=0.17764323949813843
Epoch 1: train_loss=0.4694160223007202
Epoch 2: val_loss=0.19878140091896057
Epoch 2: train_loss=0.45192426443099976
Epoch 3: val_loss=0.22101469337940216
Epoch 3: train_loss=0.44513389468193054
Epoch 4: val_loss=0.23121604323387146
Epoch 4: train_loss=0.43984028697013855
Epoch 5: val_loss=0.22473526000976562
Epoch 5: train_loss=0.4352644979953766
Epoch 6: val_loss=0.22249837219715118
Epoch 6: train_loss=0.43266552686691284
Epoch 7: val_loss=0.21166196465492249
Epoch 7: train_loss=0.4300313889980316
Epoch 8: val_loss=0.21224209666252136
Epoch 8: train_loss=0.4293772876262665
Epoch 9: val_loss=0.20762749016284943
Epoch 9: train_loss=0.42846283316612244
Epoch 10: val_loss=0.20175138115882874
Epoch 10: train_loss=0.42831480503082275


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep_baseline/checkpoints/tft-epoch=00-val_loss=0.1370.ckpt
Best validation loss: 0.137026
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 1.8 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 9.8 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.8 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 4.3 K 
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 4.3 K 
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 4.3 K 
10 | static_context_enrichment          | GatedResidualNetwork            | 4.3 K 
11 | lstm_encoder                       | LSTM                            | 8.4 K 
12 | lstm_decoder                       | LSTM                            | 8.4 K 
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.1 K 
14 | post_lstm_add_norm_encoder         | AddNorm                         | 64    
15 | static_enrichment                  | GatedResidualNetwork            | 5.3 K 
16 | multihead_attn                     | InterpretableMultiHeadAttention | 3.2 K 
17 | post_attn_gate_norm                | GateAddNorm                     | 2.2 K 
18 | pos_wise_ff                        | GatedResidualNetwork            | 4.3 K 
19 | pre_output_gate_norm               | GateAddNorm                     | 2.2 K 
20 | output_layer                       | Linear                          | 231   
----------------------------------------------------------------------------------------
67.0 K    Trainable params
0         Non-trainable params
67.0 K    Total params
0.268     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep_lr_0.0003
======================================================================
Configuration saved to: experiments/sweep_lr_0.0003/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 66,975

Starting training...
Checkpoints will be saved to: experiments/sweep_lr_0.0003/checkpoints/
Logs will be saved to: experiments/sweep_lr_0.0003/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.24047957360744476
Epoch 0: train_loss=0.708849310874939
Epoch 1: val_loss=0.1373826563358307
Epoch 1: train_loss=0.5698584318161011
Epoch 2: val_loss=0.1615651547908783
Epoch 2: train_loss=0.4852769672870636
Epoch 3: val_loss=0.17930251359939575
Epoch 3: train_loss=0.4611193537712097
Epoch 4: val_loss=0.1877153217792511
Epoch 4: train_loss=0.455287903547287
Epoch 5: val_loss=0.1962185800075531
Epoch 5: train_loss=0.4511149525642395
Epoch 6: val_loss=0.2152928113937378
Epoch 6: train_loss=0.4479774236679077
Epoch 7: val_loss=0.21827082335948944
Epoch 7: train_loss=0.44327592849731445
Epoch 8: val_loss=0.2251264750957489
Epoch 8: train_loss=0.4416790008544922
Epoch 9: val_loss=0.2267092764377594
Epoch 9: train_loss=0.4403586685657501
Epoch 10: val_loss=0.2269655168056488
Epoch 10: train_loss=0.43972083926200867
Epoch 11: val_loss=0.22614972293376923
Epoch 11: train_loss=0.4375358819961548


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep_lr_0.0003/checkpoints/tft-epoch=01-val_loss=0.1374.ckpt
Best validation loss: 0.137383
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 1.8 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 9.8 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.8 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 4.3 K 
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 4.3 K 
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 4.3 K 
10 | static_context_enrichment          | GatedResidualNetwork            | 4.3 K 
11 | lstm_encoder                       | LSTM                            | 8.4 K 
12 | lstm_decoder                       | LSTM                            | 8.4 K 
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.1 K 
14 | post_lstm_add_norm_encoder         | AddNorm                         | 64    
15 | static_enrichment                  | GatedResidualNetwork            | 5.3 K 
16 | multihead_attn                     | InterpretableMultiHeadAttention | 3.2 K 
17 | post_attn_gate_norm                | GateAddNorm                     | 2.2 K 
18 | pos_wise_ff                        | GatedResidualNetwork            | 4.3 K 
19 | pre_output_gate_norm               | GateAddNorm                     | 2.2 K 
20 | output_layer                       | Linear                          | 231   
----------------------------------------------------------------------------------------
67.0 K    Trainable params
0         Non-trainable params
67.0 K    Total params
0.268     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep_lr_0.0005
======================================================================
Configuration saved to: experiments/sweep_lr_0.0005/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 66,975

Starting training...
Checkpoints will be saved to: experiments/sweep_lr_0.0005/checkpoints/
Logs will be saved to: experiments/sweep_lr_0.0005/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.15444537997245789
Epoch 0: train_loss=0.6829169988632202
Epoch 1: val_loss=0.1546313613653183
Epoch 1: train_loss=0.5142194032669067
Epoch 2: val_loss=0.18092483282089233
Epoch 2: train_loss=0.4624931216239929
Epoch 3: val_loss=0.19113919138908386
Epoch 3: train_loss=0.45298197865486145
Epoch 4: val_loss=0.2149631530046463
Epoch 4: train_loss=0.44882732629776
Epoch 5: val_loss=0.2294456660747528
Epoch 5: train_loss=0.4436875283718109
Epoch 6: val_loss=0.23369944095611572
Epoch 6: train_loss=0.440765917301178
Epoch 7: val_loss=0.2324238419532776
Epoch 7: train_loss=0.43724533915519714
Epoch 8: val_loss=0.23527224361896515
Epoch 8: train_loss=0.43611064553260803
Epoch 9: val_loss=0.23223920166492462
Epoch 9: train_loss=0.4349413812160492
Epoch 10: val_loss=0.2185591608285904
Epoch 10: train_loss=0.4343603551387787


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep_lr_0.0005/checkpoints/tft-epoch=00-val_loss=0.1544.ckpt
Best validation loss: 0.154445
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 1.8 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 9.8 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.8 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 4.3 K 
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 4.3 K 
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 4.3 K 
10 | static_context_enrichment          | GatedResidualNetwork            | 4.3 K 
11 | lstm_encoder                       | LSTM                            | 8.4 K 
12 | lstm_decoder                       | LSTM                            | 8.4 K 
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.1 K 
14 | post_lstm_add_norm_encoder         | AddNorm                         | 64    
15 | static_enrichment                  | GatedResidualNetwork            | 5.3 K 
16 | multihead_attn                     | InterpretableMultiHeadAttention | 3.2 K 
17 | post_attn_gate_norm                | GateAddNorm                     | 2.2 K 
18 | pos_wise_ff                        | GatedResidualNetwork            | 4.3 K 
19 | pre_output_gate_norm               | GateAddNorm                     | 2.2 K 
20 | output_layer                       | Linear                          | 231   
----------------------------------------------------------------------------------------
67.0 K    Trainable params
0         Non-trainable params
67.0 K    Total params
0.268     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep_lr_0.0007
======================================================================
Configuration saved to: experiments/sweep_lr_0.0007/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 66,975

Starting training...
Checkpoints will be saved to: experiments/sweep_lr_0.0007/checkpoints/
Logs will be saved to: experiments/sweep_lr_0.0007/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.14660105109214783
Epoch 0: train_loss=0.6602282524108887
Epoch 1: val_loss=0.169380784034729
Epoch 1: train_loss=0.4876146614551544
Epoch 2: val_loss=0.18699750304222107
Epoch 2: train_loss=0.4561498463153839
Epoch 3: val_loss=0.2045581042766571
Epoch 3: train_loss=0.44933831691741943
Epoch 4: val_loss=0.23450227081775665
Epoch 4: train_loss=0.44443127512931824
Epoch 5: val_loss=0.23459839820861816
Epoch 5: train_loss=0.4394516348838806
Epoch 6: val_loss=0.23524804413318634
Epoch 6: train_loss=0.43660804629325867
Epoch 7: val_loss=0.22710677981376648
Epoch 7: train_loss=0.433562308549881
Epoch 8: val_loss=0.2292039692401886
Epoch 8: train_loss=0.4326937198638916
Epoch 9: val_loss=0.22019881010055542
Epoch 9: train_loss=0.4315653145313263
Epoch 10: val_loss=0.20812439918518066
Epoch 10: train_loss=0.431151807308197


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep_lr_0.0007/checkpoints/tft-epoch=00-val_loss=0.1466.ckpt
Best validation loss: 0.146601
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 1.8 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 9.8 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.8 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 4.3 K 
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 4.3 K 
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 4.3 K 
10 | static_context_enrichment          | GatedResidualNetwork            | 4.3 K 
11 | lstm_encoder                       | LSTM                            | 8.4 K 
12 | lstm_decoder                       | LSTM                            | 8.4 K 
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.1 K 
14 | post_lstm_add_norm_encoder         | AddNorm                         | 64    
15 | static_enrichment                  | GatedResidualNetwork            | 5.3 K 
16 | multihead_attn                     | InterpretableMultiHeadAttention | 3.2 K 
17 | post_attn_gate_norm                | GateAddNorm                     | 2.2 K 
18 | pos_wise_ff                        | GatedResidualNetwork            | 4.3 K 
19 | pre_output_gate_norm               | GateAddNorm                     | 2.2 K 
20 | output_layer                       | Linear                          | 231   
----------------------------------------------------------------------------------------
67.0 K    Trainable params
0         Non-trainable params
67.0 K    Total params
0.268     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep_lr_0.001
======================================================================
Configuration saved to: experiments/sweep_lr_0.001/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 66,975

Starting training...
Checkpoints will be saved to: experiments/sweep_lr_0.001/checkpoints/
Logs will be saved to: experiments/sweep_lr_0.001/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.13702630996704102
Epoch 0: train_loss=0.6329951286315918
Epoch 1: val_loss=0.17764323949813843
Epoch 1: train_loss=0.4694160223007202
Epoch 2: val_loss=0.19878140091896057
Epoch 2: train_loss=0.45192426443099976
Epoch 3: val_loss=0.22101469337940216
Epoch 3: train_loss=0.44513389468193054
Epoch 4: val_loss=0.23121604323387146
Epoch 4: train_loss=0.43984028697013855
Epoch 5: val_loss=0.22473526000976562
Epoch 5: train_loss=0.4352644979953766
Epoch 6: val_loss=0.22249837219715118
Epoch 6: train_loss=0.43266552686691284
Epoch 7: val_loss=0.21166196465492249
Epoch 7: train_loss=0.4300313889980316
Epoch 8: val_loss=0.21224209666252136
Epoch 8: train_loss=0.4293772876262665
Epoch 9: val_loss=0.20762749016284943
Epoch 9: train_loss=0.42846283316612244
Epoch 10: val_loss=0.20175138115882874
Epoch 10: train_loss=0.42831480503082275


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep_lr_0.001/checkpoints/tft-epoch=00-val_loss=0.1370.ckpt
Best validation loss: 0.137026
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 1.8 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 9.8 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.8 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 4.3 K 
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 4.3 K 
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 4.3 K 
10 | static_context_enrichment          | GatedResidualNetwork            | 4.3 K 
11 | lstm_encoder                       | LSTM                            | 8.4 K 
12 | lstm_decoder                       | LSTM                            | 8.4 K 
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.1 K 
14 | post_lstm_add_norm_encoder         | AddNorm                         | 64    
15 | static_enrichment                  | GatedResidualNetwork            | 5.3 K 
16 | multihead_attn                     | InterpretableMultiHeadAttention | 3.2 K 
17 | post_attn_gate_norm                | GateAddNorm                     | 2.2 K 
18 | pos_wise_ff                        | GatedResidualNetwork            | 4.3 K 
19 | pre_output_gate_norm               | GateAddNorm                     | 2.2 K 
20 | output_layer                       | Linear                          | 231   
----------------------------------------------------------------------------------------
67.0 K    Trainable params
0         Non-trainable params
67.0 K    Total params
0.268     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep_lr_0.002
======================================================================
Configuration saved to: experiments/sweep_lr_0.002/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 66,975

Starting training...
Checkpoints will be saved to: experiments/sweep_lr_0.002/checkpoints/
Logs will be saved to: experiments/sweep_lr_0.002/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.17011156678199768
Epoch 0: train_loss=0.5805390477180481
Epoch 1: val_loss=0.19511625170707703
Epoch 1: train_loss=0.45436206459999084
Epoch 2: val_loss=0.231560617685318
Epoch 2: train_loss=0.4446987807750702
Epoch 3: val_loss=0.2396521270275116
Epoch 3: train_loss=0.43687134981155396
Epoch 4: val_loss=0.24400106072425842
Epoch 4: train_loss=0.43245571851730347
Epoch 5: val_loss=0.2207532525062561
Epoch 5: train_loss=0.42922478914260864
Epoch 6: val_loss=0.22538578510284424
Epoch 6: train_loss=0.4272315204143524
Epoch 7: val_loss=0.2194879949092865
Epoch 7: train_loss=0.42525243759155273
Epoch 8: val_loss=0.22214120626449585
Epoch 8: train_loss=0.4247385859489441
Epoch 9: val_loss=0.2166716307401657
Epoch 9: train_loss=0.4246326982975006
Epoch 10: val_loss=0.22192412614822388
Epoch 10: train_loss=0.42462897300720215


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep_lr_0.002/checkpoints/tft-epoch=00-val_loss=0.1701.ckpt
Best validation loss: 0.170112
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 1.2 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 6.4 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.2 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K 
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K 
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K 
10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K 
11 | lstm_encoder                       | LSTM                            | 2.2 K 
12 | lstm_decoder                       | LSTM                            | 2.2 K 
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   
14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    
15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K 
16 | multihead_attn                     | InterpretableMultiHeadAttention | 808   
17 | post_attn_gate_norm                | GateAddNorm                     | 576   
18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K 
19 | pre_output_gate_norm               | GateAddNorm                     | 576   
20 | output_layer                       | Linear                          | 119   
----------------------------------------------------------------------------------------
22.6 K    Trainable params
0         Non-trainable params
22.6 K    Total params
0.090     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep_hidden_16
======================================================================
Configuration saved to: experiments/sweep_hidden_16/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 22,615

Starting training...
Checkpoints will be saved to: experiments/sweep_hidden_16/checkpoints/
Logs will be saved to: experiments/sweep_hidden_16/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.3859482407569885
Epoch 0: train_loss=0.8888093829154968
Epoch 1: val_loss=0.13244730234146118
Epoch 1: train_loss=0.6538586020469666
Epoch 2: val_loss=0.15113747119903564
Epoch 2: train_loss=0.5082658529281616
Epoch 3: val_loss=0.1772746443748474
Epoch 3: train_loss=0.4686850607395172
Epoch 4: val_loss=0.1867443174123764
Epoch 4: train_loss=0.45871999859809875
Epoch 5: val_loss=0.19207289814949036
Epoch 5: train_loss=0.45595797896385193
Epoch 6: val_loss=0.18732020258903503
Epoch 6: train_loss=0.45375433564186096
Epoch 7: val_loss=0.19324584305286407
Epoch 7: train_loss=0.44986245036125183
Epoch 8: val_loss=0.2022390067577362
Epoch 8: train_loss=0.4503166079521179
Epoch 9: val_loss=0.20807136595249176
Epoch 9: train_loss=0.4487007260322571
Epoch 10: val_loss=0.21224766969680786
Epoch 10: train_loss=0.4468279778957367
Epoch 11: val_loss=0.21922841668128967
Epoch 11: train_loss=0.44511479139328003


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep_hidden_16/checkpoints/tft-epoch=01-val_loss=0.1324.ckpt
Best validation loss: 0.132447
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 1.5 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 8.2 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.5 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 2.4 K 
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 2.4 K 
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 2.4 K 
10 | static_context_enrichment          | GatedResidualNetwork            | 2.4 K 
11 | lstm_encoder                       | LSTM                            | 4.8 K 
12 | lstm_decoder                       | LSTM                            | 4.8 K 
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 1.2 K 
14 | post_lstm_add_norm_encoder         | AddNorm                         | 48    
15 | static_enrichment                  | GatedResidualNetwork            | 3.0 K 
16 | multihead_attn                     | InterpretableMultiHeadAttention | 1.8 K 
17 | post_attn_gate_norm                | GateAddNorm                     | 1.2 K 
18 | pos_wise_ff                        | GatedResidualNetwork            | 2.4 K 
19 | pre_output_gate_norm               | GateAddNorm                     | 1.2 K 
20 | output_layer                       | Linear                          | 175   
----------------------------------------------------------------------------------------
41.8 K    Trainable params
0         Non-trainable params
41.8 K    Total params
0.167     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep_hidden_24
======================================================================
Configuration saved to: experiments/sweep_hidden_24/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 41,763

Starting training...
Checkpoints will be saved to: experiments/sweep_hidden_24/checkpoints/
Logs will be saved to: experiments/sweep_hidden_24/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.10557609796524048
Epoch 0: train_loss=0.7348565459251404
Epoch 1: val_loss=0.1609610915184021
Epoch 1: train_loss=0.5506575703620911
Epoch 2: val_loss=0.18162111937999725
Epoch 2: train_loss=0.47207990288734436
Epoch 3: val_loss=0.18000636994838715
Epoch 3: train_loss=0.45658013224601746
Epoch 4: val_loss=0.19861844182014465
Epoch 4: train_loss=0.45103171467781067
Epoch 5: val_loss=0.2257538139820099
Epoch 5: train_loss=0.4457472562789917
Epoch 6: val_loss=0.22860227525234222
Epoch 6: train_loss=0.44157400727272034
Epoch 7: val_loss=0.2353048324584961
Epoch 7: train_loss=0.44089454412460327
Epoch 8: val_loss=0.25287002325057983
Epoch 8: train_loss=0.440006822347641
Epoch 9: val_loss=0.25338214635849
Epoch 9: train_loss=0.43776100873947144
Epoch 10: val_loss=0.25444528460502625
Epoch 10: train_loss=0.43649160861968994


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep_hidden_24/checkpoints/tft-epoch=00-val_loss=0.1056.ckpt
Best validation loss: 0.105576
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 1.8 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 9.8 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.8 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 4.3 K 
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 4.3 K 
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 4.3 K 
10 | static_context_enrichment          | GatedResidualNetwork            | 4.3 K 
11 | lstm_encoder                       | LSTM                            | 8.4 K 
12 | lstm_decoder                       | LSTM                            | 8.4 K 
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.1 K 
14 | post_lstm_add_norm_encoder         | AddNorm                         | 64    
15 | static_enrichment                  | GatedResidualNetwork            | 5.3 K 
16 | multihead_attn                     | InterpretableMultiHeadAttention | 3.2 K 
17 | post_attn_gate_norm                | GateAddNorm                     | 2.2 K 
18 | pos_wise_ff                        | GatedResidualNetwork            | 4.3 K 
19 | pre_output_gate_norm               | GateAddNorm                     | 2.2 K 
20 | output_layer                       | Linear                          | 231   
----------------------------------------------------------------------------------------
67.0 K    Trainable params
0         Non-trainable params
67.0 K    Total params
0.268     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep_hidden_32
======================================================================
Configuration saved to: experiments/sweep_hidden_32/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 66,975

Starting training...
Checkpoints will be saved to: experiments/sweep_hidden_32/checkpoints/
Logs will be saved to: experiments/sweep_hidden_32/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.15444537997245789
Epoch 0: train_loss=0.6829169988632202
Epoch 1: val_loss=0.1546313613653183
Epoch 1: train_loss=0.5142194032669067
Epoch 2: val_loss=0.18092483282089233
Epoch 2: train_loss=0.4624931216239929
Epoch 3: val_loss=0.19113919138908386
Epoch 3: train_loss=0.45298197865486145
Epoch 4: val_loss=0.2149631530046463
Epoch 4: train_loss=0.44882732629776
Epoch 5: val_loss=0.2294456660747528
Epoch 5: train_loss=0.4436875283718109
Epoch 6: val_loss=0.23369944095611572
Epoch 6: train_loss=0.440765917301178
Epoch 7: val_loss=0.2324238419532776
Epoch 7: train_loss=0.43724533915519714
Epoch 8: val_loss=0.23527224361896515
Epoch 8: train_loss=0.43611064553260803
Epoch 9: val_loss=0.23223920166492462
Epoch 9: train_loss=0.4349413812160492
Epoch 10: val_loss=0.2185591608285904
Epoch 10: train_loss=0.4343603551387787


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep_hidden_32/checkpoints/tft-epoch=00-val_loss=0.1544.ckpt
Best validation loss: 0.154445
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 2.4 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 13.0 K
6  | decoder_variable_selection         | VariableSelectionNetwork        | 2.4 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 9.5 K 
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 9.5 K 
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 9.5 K 
10 | static_context_enrichment          | GatedResidualNetwork            | 9.5 K 
11 | lstm_encoder                       | LSTM                            | 18.8 K
12 | lstm_decoder                       | LSTM                            | 18.8 K
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 4.7 K 
14 | post_lstm_add_norm_encoder         | AddNorm                         | 96    
15 | static_enrichment                  | GatedResidualNetwork            | 11.8 K
16 | multihead_attn                     | InterpretableMultiHeadAttention | 7.0 K 
17 | post_attn_gate_norm                | GateAddNorm                     | 4.8 K 
18 | pos_wise_ff                        | GatedResidualNetwork            | 9.5 K 
19 | pre_output_gate_norm               | GateAddNorm                     | 4.8 K 
20 | output_layer                       | Linear                          | 343   
----------------------------------------------------------------------------------------
136 K     Trainable params
0         Non-trainable params
136 K     Total params
0.546     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep_hidden_48
======================================================================
Configuration saved to: experiments/sweep_hidden_48/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 136,599

Starting training...
Checkpoints will be saved to: experiments/sweep_hidden_48/checkpoints/
Logs will be saved to: experiments/sweep_hidden_48/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.22129863500595093
Epoch 0: train_loss=0.8424229621887207
Epoch 1: val_loss=0.15297546982765198
Epoch 1: train_loss=0.5431578159332275
Epoch 2: val_loss=0.1759035885334015
Epoch 2: train_loss=0.4598625600337982
Epoch 3: val_loss=0.21826523542404175
Epoch 3: train_loss=0.44842833280563354
Epoch 4: val_loss=0.24683228135108948
Epoch 4: train_loss=0.44208571314811707
Epoch 5: val_loss=0.23417893052101135
Epoch 5: train_loss=0.44014501571655273
Epoch 6: val_loss=0.2292252779006958
Epoch 6: train_loss=0.43553847074508667
Epoch 7: val_loss=0.24168094992637634
Epoch 7: train_loss=0.43358010053634644
Epoch 8: val_loss=0.24970947206020355
Epoch 8: train_loss=0.43345460295677185
Epoch 9: val_loss=0.23556914925575256
Epoch 9: train_loss=0.43227508664131165
Epoch 10: val_loss=0.24147726595401764
Epoch 10: train_loss=0.4302871823310852
Epoch 11: val_loss=0.22617878019809723
Epoch 11: train_loss=0.43002769351005554


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep_hidden_48/checkpoints/tft-epoch=01-val_loss=0.1530.ckpt
Best validation loss: 0.152975
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 3.1 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 16.2 K
6  | decoder_variable_selection         | VariableSelectionNetwork        | 3.1 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 16.8 K
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 16.8 K
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 16.8 K
10 | static_context_enrichment          | GatedResidualNetwork            | 16.8 K
11 | lstm_encoder                       | LSTM                            | 33.3 K
12 | lstm_decoder                       | LSTM                            | 33.3 K
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 8.3 K 
14 | post_lstm_add_norm_encoder         | AddNorm                         | 128   
15 | static_enrichment                  | GatedResidualNetwork            | 20.9 K
16 | multihead_attn                     | InterpretableMultiHeadAttention | 12.4 K
17 | post_attn_gate_norm                | GateAddNorm                     | 8.4 K 
18 | pos_wise_ff                        | GatedResidualNetwork            | 16.8 K
19 | pre_output_gate_norm               | GateAddNorm                     | 8.4 K 
20 | output_layer                       | Linear                          | 455   
----------------------------------------------------------------------------------------
231 K     Trainable params
0         Non-trainable params
231 K     Total params
0.927     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep_hidden_64
======================================================================
Configuration saved to: experiments/sweep_hidden_64/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 231,823

Starting training...
Checkpoints will be saved to: experiments/sweep_hidden_64/checkpoints/
Logs will be saved to: experiments/sweep_hidden_64/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.16334760189056396
Epoch 0: train_loss=0.9102017879486084
Epoch 1: val_loss=0.18439921736717224
Epoch 1: train_loss=0.5227071046829224
Epoch 2: val_loss=0.21810267865657806
Epoch 2: train_loss=0.45310819149017334
Epoch 3: val_loss=0.24974310398101807
Epoch 3: train_loss=0.4413872957229614
Epoch 4: val_loss=0.29438215494155884
Epoch 4: train_loss=0.4369398057460785
Epoch 5: val_loss=0.29824140667915344
Epoch 5: train_loss=0.43474501371383667
Epoch 6: val_loss=0.2929758131504059
Epoch 6: train_loss=0.432060569524765
Epoch 7: val_loss=0.30674293637275696
Epoch 7: train_loss=0.428871750831604
Epoch 8: val_loss=0.3185945153236389
Epoch 8: train_loss=0.42966046929359436
Epoch 9: val_loss=0.3233926296234131
Epoch 9: train_loss=0.4290362000465393
Epoch 10: val_loss=0.30893123149871826
Epoch 10: train_loss=0.42922213673591614


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep_hidden_64/checkpoints/tft-epoch=00-val_loss=0.1633.ckpt
Best validation loss: 0.163348
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 1.8 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 9.8 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.8 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 4.3 K 
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 4.3 K 
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 4.3 K 
10 | static_context_enrichment          | GatedResidualNetwork            | 4.3 K 
11 | lstm_encoder                       | LSTM                            | 8.4 K 
12 | lstm_decoder                       | LSTM                            | 8.4 K 
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.1 K 
14 | post_lstm_add_norm_encoder         | AddNorm                         | 64    
15 | static_enrichment                  | GatedResidualNetwork            | 5.3 K 
16 | multihead_attn                     | InterpretableMultiHeadAttention | 3.2 K 
17 | post_attn_gate_norm                | GateAddNorm                     | 2.2 K 
18 | pos_wise_ff                        | GatedResidualNetwork            | 4.3 K 
19 | pre_output_gate_norm               | GateAddNorm                     | 2.2 K 
20 | output_layer                       | Linear                          | 231   
----------------------------------------------------------------------------------------
67.0 K    Trainable params
0         Non-trainable params
67.0 K    Total params
0.268     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep_encoder_10
======================================================================
Configuration saved to: experiments/sweep_encoder_10/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 66,975

Starting training...
Checkpoints will be saved to: experiments/sweep_encoder_10/checkpoints/
Logs will be saved to: experiments/sweep_encoder_10/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.152070090174675
Epoch 0: train_loss=0.6855782866477966
Epoch 1: val_loss=0.15662111341953278
Epoch 1: train_loss=0.5145572423934937
Epoch 2: val_loss=0.17671999335289001
Epoch 2: train_loss=0.46255233883857727
Epoch 3: val_loss=0.1829642653465271
Epoch 3: train_loss=0.45370909571647644
Epoch 4: val_loss=0.21463122963905334
Epoch 4: train_loss=0.44862616062164307
Epoch 5: val_loss=0.22706733644008636
Epoch 5: train_loss=0.44314464926719666
Epoch 6: val_loss=0.24004456400871277
Epoch 6: train_loss=0.4386773705482483
Epoch 7: val_loss=0.24028024077415466
Epoch 7: train_loss=0.43734559416770935
Epoch 8: val_loss=0.23578262329101562
Epoch 8: train_loss=0.43577542901039124
Epoch 9: val_loss=0.23720860481262207
Epoch 9: train_loss=0.4344014823436737
Epoch 10: val_loss=0.22971338033676147
Epoch 10: train_loss=0.43391910195350647


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep_encoder_10/checkpoints/tft-epoch=00-val_loss=0.1521.ckpt
Best validation loss: 0.152070
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 1.8 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 9.8 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.8 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 4.3 K 
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 4.3 K 
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 4.3 K 
10 | static_context_enrichment          | GatedResidualNetwork            | 4.3 K 
11 | lstm_encoder                       | LSTM                            | 8.4 K 
12 | lstm_decoder                       | LSTM                            | 8.4 K 
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.1 K 
14 | post_lstm_add_norm_encoder         | AddNorm                         | 64    
15 | static_enrichment                  | GatedResidualNetwork            | 5.3 K 
16 | multihead_attn                     | InterpretableMultiHeadAttention | 3.2 K 
17 | post_attn_gate_norm                | GateAddNorm                     | 2.2 K 
18 | pos_wise_ff                        | GatedResidualNetwork            | 4.3 K 
19 | pre_output_gate_norm               | GateAddNorm                     | 2.2 K 
20 | output_layer                       | Linear                          | 231   
----------------------------------------------------------------------------------------
67.0 K    Trainable params
0         Non-trainable params
67.0 K    Total params
0.268     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep_encoder_20
======================================================================
Configuration saved to: experiments/sweep_encoder_20/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 66,975

Starting training...
Checkpoints will be saved to: experiments/sweep_encoder_20/checkpoints/
Logs will be saved to: experiments/sweep_encoder_20/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.15444537997245789
Epoch 0: train_loss=0.6829169988632202
Epoch 1: val_loss=0.1546313613653183
Epoch 1: train_loss=0.5142194032669067
Epoch 2: val_loss=0.18092483282089233
Epoch 2: train_loss=0.4624931216239929
Epoch 3: val_loss=0.19113919138908386
Epoch 3: train_loss=0.45298197865486145
Epoch 4: val_loss=0.2149631530046463
Epoch 4: train_loss=0.44882732629776
Epoch 5: val_loss=0.2294456660747528
Epoch 5: train_loss=0.4436875283718109
Epoch 6: val_loss=0.23369944095611572
Epoch 6: train_loss=0.440765917301178
Epoch 7: val_loss=0.2324238419532776
Epoch 7: train_loss=0.43724533915519714
Epoch 8: val_loss=0.23527224361896515
Epoch 8: train_loss=0.43611064553260803
Epoch 9: val_loss=0.23223920166492462
Epoch 9: train_loss=0.4349413812160492
Epoch 10: val_loss=0.2185591608285904
Epoch 10: train_loss=0.4343603551387787


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep_encoder_20/checkpoints/tft-epoch=00-val_loss=0.1544.ckpt
Best validation loss: 0.154445
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 1.8 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 9.8 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.8 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 4.3 K 
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 4.3 K 
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 4.3 K 
10 | static_context_enrichment          | GatedResidualNetwork            | 4.3 K 
11 | lstm_encoder                       | LSTM                            | 8.4 K 
12 | lstm_decoder                       | LSTM                            | 8.4 K 
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.1 K 
14 | post_lstm_add_norm_encoder         | AddNorm                         | 64    
15 | static_enrichment                  | GatedResidualNetwork            | 5.3 K 
16 | multihead_attn                     | InterpretableMultiHeadAttention | 3.2 K 
17 | post_attn_gate_norm                | GateAddNorm                     | 2.2 K 
18 | pos_wise_ff                        | GatedResidualNetwork            | 4.3 K 
19 | pre_output_gate_norm               | GateAddNorm                     | 2.2 K 
20 | output_layer                       | Linear                          | 231   
----------------------------------------------------------------------------------------
67.0 K    Trainable params
0         Non-trainable params
67.0 K    Total params
0.268     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep_encoder_40
======================================================================
Configuration saved to: experiments/sweep_encoder_40/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 66,975

Starting training...
Checkpoints will be saved to: experiments/sweep_encoder_40/checkpoints/
Logs will be saved to: experiments/sweep_encoder_40/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.15325002372264862
Epoch 0: train_loss=0.6881154775619507
Epoch 1: val_loss=0.15735505521297455
Epoch 1: train_loss=0.51568204164505
Epoch 2: val_loss=0.18038292229175568
Epoch 2: train_loss=0.46280694007873535
Epoch 3: val_loss=0.19280469417572021
Epoch 3: train_loss=0.4541628956794739
Epoch 4: val_loss=0.20610539615154266
Epoch 4: train_loss=0.4495333433151245
Epoch 5: val_loss=0.22595329582691193
Epoch 5: train_loss=0.444033145904541
Epoch 6: val_loss=0.2211974561214447
Epoch 6: train_loss=0.44068560004234314
Epoch 7: val_loss=0.22767898440361023
Epoch 7: train_loss=0.4393266439437866
Epoch 8: val_loss=0.2205783873796463
Epoch 8: train_loss=0.43756070733070374
Epoch 9: val_loss=0.2196199595928192
Epoch 9: train_loss=0.4364508092403412
Epoch 10: val_loss=0.21332749724388123
Epoch 10: train_loss=0.4354911148548126


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep_encoder_40/checkpoints/tft-epoch=00-val_loss=0.1533.ckpt
Best validation loss: 0.153250
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 1.8 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 9.8 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.8 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 4.3 K 
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 4.3 K 
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 4.3 K 
10 | static_context_enrichment          | GatedResidualNetwork            | 4.3 K 
11 | lstm_encoder                       | LSTM                            | 8.4 K 
12 | lstm_decoder                       | LSTM                            | 8.4 K 
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.1 K 
14 | post_lstm_add_norm_encoder         | AddNorm                         | 64    
15 | static_enrichment                  | GatedResidualNetwork            | 5.3 K 
16 | multihead_attn                     | InterpretableMultiHeadAttention | 3.2 K 
17 | post_attn_gate_norm                | GateAddNorm                     | 2.2 K 
18 | pos_wise_ff                        | GatedResidualNetwork            | 4.3 K 
19 | pre_output_gate_norm               | GateAddNorm                     | 2.2 K 
20 | output_layer                       | Linear                          | 231   
----------------------------------------------------------------------------------------
67.0 K    Trainable params
0         Non-trainable params
67.0 K    Total params
0.268     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep_encoder_60
======================================================================
Configuration saved to: experiments/sweep_encoder_60/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 93

Initializing model...
Model parameters: 66,975

Starting training...
Checkpoints will be saved to: experiments/sweep_encoder_60/checkpoints/
Logs will be saved to: experiments/sweep_encoder_60/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.15337315201759338
Epoch 0: train_loss=0.6886752843856812
Epoch 1: val_loss=0.15357615053653717
Epoch 1: train_loss=0.5166827440261841
Epoch 2: val_loss=0.1770927906036377
Epoch 2: train_loss=0.4630783796310425
Epoch 3: val_loss=0.17999690771102905
Epoch 3: train_loss=0.45499539375305176
Epoch 4: val_loss=0.2026555836200714
Epoch 4: train_loss=0.4487439692020416
Epoch 5: val_loss=0.21896576881408691
Epoch 5: train_loss=0.4440549314022064
Epoch 6: val_loss=0.2285814881324768
Epoch 6: train_loss=0.44246381521224976
Epoch 7: val_loss=0.2310519963502884
Epoch 7: train_loss=0.4401429295539856
Epoch 8: val_loss=0.22705301642417908
Epoch 8: train_loss=0.43859195709228516
Epoch 9: val_loss=0.22211295366287231
Epoch 9: train_loss=0.43618565797805786
Epoch 10: val_loss=0.22250166535377502
Epoch 10: train_loss=0.4362635016441345


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep_encoder_60/checkpoints/tft-epoch=00-val_loss=0.1534.ckpt
Best validation loss: 0.153373
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 1.8 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 9.8 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.8 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 4.3 K 
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 4.3 K 
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 4.3 K 
10 | static_context_enrichment          | GatedResidualNetwork            | 4.3 K 
11 | lstm_encoder                       | LSTM                            | 8.4 K 
12 | lstm_decoder                       | LSTM                            | 8.4 K 
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.1 K 
14 | post_lstm_add_norm_encoder         | AddNorm                         | 64    
15 | static_enrichment                  | GatedResidualNetwork            | 5.3 K 
16 | multihead_attn                     | InterpretableMultiHeadAttention | 3.2 K 
17 | post_attn_gate_norm                | GateAddNorm                     | 2.2 K 
18 | pos_wise_ff                        | GatedResidualNetwork            | 4.3 K 
19 | pre_output_gate_norm               | GateAddNorm                     | 2.2 K 
20 | output_layer                       | Linear                          | 231   
----------------------------------------------------------------------------------------
67.0 K    Trainable params
0         Non-trainable params
67.0 K    Total params
0.268     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep_dropout_0.1
======================================================================
Configuration saved to: experiments/sweep_dropout_0.1/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 66,975

Starting training...
Checkpoints will be saved to: experiments/sweep_dropout_0.1/checkpoints/
Logs will be saved to: experiments/sweep_dropout_0.1/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.1530822515487671
Epoch 0: train_loss=0.682960033416748
Epoch 1: val_loss=0.15359213948249817
Epoch 1: train_loss=0.5107463002204895
Epoch 2: val_loss=0.1771513968706131
Epoch 2: train_loss=0.46060800552368164
Epoch 3: val_loss=0.19088909029960632
Epoch 3: train_loss=0.4516771733760834
Epoch 4: val_loss=0.21560615301132202
Epoch 4: train_loss=0.4471518397331238
Epoch 5: val_loss=0.2252296656370163
Epoch 5: train_loss=0.44178977608680725
Epoch 6: val_loss=0.2256661355495453
Epoch 6: train_loss=0.4388220012187958
Epoch 7: val_loss=0.22062933444976807
Epoch 7: train_loss=0.4354245662689209
Epoch 8: val_loss=0.22223056852817535
Epoch 8: train_loss=0.43463099002838135
Epoch 9: val_loss=0.2159266620874405
Epoch 9: train_loss=0.4335218071937561
Epoch 10: val_loss=0.20607562363147736
Epoch 10: train_loss=0.43241748213768005


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep_dropout_0.1/checkpoints/tft-epoch=00-val_loss=0.1531.ckpt
Best validation loss: 0.153082
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 1.8 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 9.8 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.8 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 4.3 K 
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 4.3 K 
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 4.3 K 
10 | static_context_enrichment          | GatedResidualNetwork            | 4.3 K 
11 | lstm_encoder                       | LSTM                            | 8.4 K 
12 | lstm_decoder                       | LSTM                            | 8.4 K 
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.1 K 
14 | post_lstm_add_norm_encoder         | AddNorm                         | 64    
15 | static_enrichment                  | GatedResidualNetwork            | 5.3 K 
16 | multihead_attn                     | InterpretableMultiHeadAttention | 3.2 K 
17 | post_attn_gate_norm                | GateAddNorm                     | 2.2 K 
18 | pos_wise_ff                        | GatedResidualNetwork            | 4.3 K 
19 | pre_output_gate_norm               | GateAddNorm                     | 2.2 K 
20 | output_layer                       | Linear                          | 231   
----------------------------------------------------------------------------------------
67.0 K    Trainable params
0         Non-trainable params
67.0 K    Total params
0.268     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep_dropout_0.15
======================================================================
Configuration saved to: experiments/sweep_dropout_0.15/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 66,975

Starting training...
Checkpoints will be saved to: experiments/sweep_dropout_0.15/checkpoints/
Logs will be saved to: experiments/sweep_dropout_0.15/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.15444537997245789
Epoch 0: train_loss=0.6829169988632202
Epoch 1: val_loss=0.1546313613653183
Epoch 1: train_loss=0.5142194032669067
Epoch 2: val_loss=0.18092483282089233
Epoch 2: train_loss=0.4624931216239929
Epoch 3: val_loss=0.19113919138908386
Epoch 3: train_loss=0.45298197865486145
Epoch 4: val_loss=0.2149631530046463
Epoch 4: train_loss=0.44882732629776
Epoch 5: val_loss=0.2294456660747528
Epoch 5: train_loss=0.4436875283718109
Epoch 6: val_loss=0.23369944095611572
Epoch 6: train_loss=0.440765917301178
Epoch 7: val_loss=0.2324238419532776
Epoch 7: train_loss=0.43724533915519714
Epoch 8: val_loss=0.23527224361896515
Epoch 8: train_loss=0.43611064553260803
Epoch 9: val_loss=0.23223920166492462
Epoch 9: train_loss=0.4349413812160492
Epoch 10: val_loss=0.2185591608285904
Epoch 10: train_loss=0.4343603551387787


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep_dropout_0.15/checkpoints/tft-epoch=00-val_loss=0.1544.ckpt
Best validation loss: 0.154445
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 1.8 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 9.8 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.8 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 4.3 K 
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 4.3 K 
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 4.3 K 
10 | static_context_enrichment          | GatedResidualNetwork            | 4.3 K 
11 | lstm_encoder                       | LSTM                            | 8.4 K 
12 | lstm_decoder                       | LSTM                            | 8.4 K 
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.1 K 
14 | post_lstm_add_norm_encoder         | AddNorm                         | 64    
15 | static_enrichment                  | GatedResidualNetwork            | 5.3 K 
16 | multihead_attn                     | InterpretableMultiHeadAttention | 3.2 K 
17 | post_attn_gate_norm                | GateAddNorm                     | 2.2 K 
18 | pos_wise_ff                        | GatedResidualNetwork            | 4.3 K 
19 | pre_output_gate_norm               | GateAddNorm                     | 2.2 K 
20 | output_layer                       | Linear                          | 231   
----------------------------------------------------------------------------------------
67.0 K    Trainable params
0         Non-trainable params
67.0 K    Total params
0.268     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep_dropout_0.2
======================================================================
Configuration saved to: experiments/sweep_dropout_0.2/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 66,975

Starting training...
Checkpoints will be saved to: experiments/sweep_dropout_0.2/checkpoints/
Logs will be saved to: experiments/sweep_dropout_0.2/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.15493205189704895
Epoch 0: train_loss=0.6837108731269836
Epoch 1: val_loss=0.15523919463157654
Epoch 1: train_loss=0.5181668996810913
Epoch 2: val_loss=0.17908453941345215
Epoch 2: train_loss=0.46424177289009094
Epoch 3: val_loss=0.18842193484306335
Epoch 3: train_loss=0.45428961515426636
Epoch 4: val_loss=0.2081979513168335
Epoch 4: train_loss=0.45049384236335754
Epoch 5: val_loss=0.2207070291042328
Epoch 5: train_loss=0.44570180773735046
Epoch 6: val_loss=0.2330903708934784
Epoch 6: train_loss=0.4425587058067322
Epoch 7: val_loss=0.23383864760398865
Epoch 7: train_loss=0.43902602791786194
Epoch 8: val_loss=0.23803822696208954
Epoch 8: train_loss=0.4380089044570923
Epoch 9: val_loss=0.23726898431777954
Epoch 9: train_loss=0.4363192021846771
Epoch 10: val_loss=0.2303289771080017
Epoch 10: train_loss=0.4356948733329773


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep_dropout_0.2/checkpoints/tft-epoch=00-val_loss=0.1549.ckpt
Best validation loss: 0.154932
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 1.8 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 9.8 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.8 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 4.3 K 
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 4.3 K 
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 4.3 K 
10 | static_context_enrichment          | GatedResidualNetwork            | 4.3 K 
11 | lstm_encoder                       | LSTM                            | 8.4 K 
12 | lstm_decoder                       | LSTM                            | 8.4 K 
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.1 K 
14 | post_lstm_add_norm_encoder         | AddNorm                         | 64    
15 | static_enrichment                  | GatedResidualNetwork            | 5.3 K 
16 | multihead_attn                     | InterpretableMultiHeadAttention | 3.2 K 
17 | post_attn_gate_norm                | GateAddNorm                     | 2.2 K 
18 | pos_wise_ff                        | GatedResidualNetwork            | 4.3 K 
19 | pre_output_gate_norm               | GateAddNorm                     | 2.2 K 
20 | output_layer                       | Linear                          | 231   
----------------------------------------------------------------------------------------
67.0 K    Trainable params
0         Non-trainable params
67.0 K    Total params
0.268     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep_dropout_0.3
======================================================================
Configuration saved to: experiments/sweep_dropout_0.3/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 66,975

Starting training...
Checkpoints will be saved to: experiments/sweep_dropout_0.3/checkpoints/
Logs will be saved to: experiments/sweep_dropout_0.3/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.1553630381822586
Epoch 0: train_loss=0.6853129863739014
Epoch 1: val_loss=0.1507239192724228
Epoch 1: train_loss=0.5272361040115356
Epoch 2: val_loss=0.17659813165664673
Epoch 2: train_loss=0.4678400456905365
Epoch 3: val_loss=0.186200812458992
Epoch 3: train_loss=0.4568259119987488
Epoch 4: val_loss=0.19746214151382446
Epoch 4: train_loss=0.4535570442676544
Epoch 5: val_loss=0.20647940039634705
Epoch 5: train_loss=0.44879215955734253
Epoch 6: val_loss=0.22622153162956238
Epoch 6: train_loss=0.44475114345550537
Epoch 7: val_loss=0.23222480714321136
Epoch 7: train_loss=0.4408397376537323
Epoch 8: val_loss=0.23700951039791107
Epoch 8: train_loss=0.4395158588886261
Epoch 9: val_loss=0.24224182963371277
Epoch 9: train_loss=0.4386548399925232
Epoch 10: val_loss=0.24096502363681793
Epoch 10: train_loss=0.4373634159564972
Epoch 11: val_loss=0.24443678557872772
Epoch 11: train_loss=0.4355696141719818


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep_dropout_0.3/checkpoints/tft-epoch=01-val_loss=0.1507.ckpt
Best validation loss: 0.150724
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 2.4 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 13.0 K
6  | decoder_variable_selection         | VariableSelectionNetwork        | 2.4 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 9.5 K 
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 9.5 K 
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 9.5 K 
10 | static_context_enrichment          | GatedResidualNetwork            | 9.5 K 
11 | lstm_encoder                       | LSTM                            | 18.8 K
12 | lstm_decoder                       | LSTM                            | 18.8 K
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 4.7 K 
14 | post_lstm_add_norm_encoder         | AddNorm                         | 96    
15 | static_enrichment                  | GatedResidualNetwork            | 11.8 K
16 | multihead_attn                     | InterpretableMultiHeadAttention | 7.0 K 
17 | post_attn_gate_norm                | GateAddNorm                     | 4.8 K 
18 | pos_wise_ff                        | GatedResidualNetwork            | 9.5 K 
19 | pre_output_gate_norm               | GateAddNorm                     | 4.8 K 
20 | output_layer                       | Linear                          | 343   
----------------------------------------------------------------------------------------
136 K     Trainable params
0         Non-trainable params
136 K     Total params
0.546     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep_combo1
======================================================================
Configuration saved to: experiments/sweep_combo1/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 93

Initializing model...
Model parameters: 136,599

Starting training...
Checkpoints will be saved to: experiments/sweep_combo1/checkpoints/
Logs will be saved to: experiments/sweep_combo1/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.2368728518486023
Epoch 0: train_loss=0.8548961281776428
Epoch 1: val_loss=0.14941918849945068
Epoch 1: train_loss=0.5540474057197571
Epoch 2: val_loss=0.16979268193244934
Epoch 2: train_loss=0.46236464381217957
Epoch 3: val_loss=0.20485317707061768
Epoch 3: train_loss=0.44751083850860596
Epoch 4: val_loss=0.212498277425766
Epoch 4: train_loss=0.4454365670681
Epoch 5: val_loss=0.21554794907569885
Epoch 5: train_loss=0.4425240755081177
Epoch 6: val_loss=0.23963278532028198
Epoch 6: train_loss=0.4390414357185364
Epoch 7: val_loss=0.22730758786201477
Epoch 7: train_loss=0.43743547797203064
Epoch 8: val_loss=0.21005049347877502
Epoch 8: train_loss=0.4353652000427246
Epoch 9: val_loss=0.2379971742630005
Epoch 9: train_loss=0.43344390392303467
Epoch 10: val_loss=0.24000422656536102
Epoch 10: train_loss=0.4301827847957611
Epoch 11: val_loss=0.2337958812713623
Epoch 11: train_loss=0.43132126331329346


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep_combo1/checkpoints/tft-epoch=01-val_loss=0.1494.ckpt
Best validation loss: 0.149419
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 3.1 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 16.2 K
6  | decoder_variable_selection         | VariableSelectionNetwork        | 3.1 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 16.8 K
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 16.8 K
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 16.8 K
10 | static_context_enrichment          | GatedResidualNetwork            | 16.8 K
11 | lstm_encoder                       | LSTM                            | 33.3 K
12 | lstm_decoder                       | LSTM                            | 33.3 K
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 8.3 K 
14 | post_lstm_add_norm_encoder         | AddNorm                         | 128   
15 | static_enrichment                  | GatedResidualNetwork            | 20.9 K
16 | multihead_attn                     | InterpretableMultiHeadAttention | 12.4 K
17 | post_attn_gate_norm                | GateAddNorm                     | 8.4 K 
18 | pos_wise_ff                        | GatedResidualNetwork            | 16.8 K
19 | pre_output_gate_norm               | GateAddNorm                     | 8.4 K 
20 | output_layer                       | Linear                          | 455   
----------------------------------------------------------------------------------------
231 K     Trainable params
0         Non-trainable params
231 K     Total params
0.927     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep_combo2
======================================================================
Configuration saved to: experiments/sweep_combo2/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 231,823

Starting training...
Checkpoints will be saved to: experiments/sweep_combo2/checkpoints/
Logs will be saved to: experiments/sweep_combo2/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.3219979703426361
Epoch 0: train_loss=0.9773350954055786
Epoch 1: val_loss=0.13855642080307007
Epoch 1: train_loss=0.6295253038406372
Epoch 2: val_loss=0.1966620534658432
Epoch 2: train_loss=0.47367367148399353
Epoch 3: val_loss=0.21988995373249054
Epoch 3: train_loss=0.45179298520088196
Epoch 4: val_loss=0.2633291780948639
Epoch 4: train_loss=0.4456574022769928
Epoch 5: val_loss=0.2787635326385498
Epoch 5: train_loss=0.43998289108276367
Epoch 6: val_loss=0.2815524637699127
Epoch 6: train_loss=0.43628010153770447
Epoch 7: val_loss=0.2782368063926697
Epoch 7: train_loss=0.4348812401294708
Epoch 8: val_loss=0.27920424938201904
Epoch 8: train_loss=0.43445366621017456
Epoch 9: val_loss=0.27678215503692627
Epoch 9: train_loss=0.43274176120758057
Epoch 10: val_loss=0.28968098759651184
Epoch 10: train_loss=0.43323010206222534
Epoch 11: val_loss=0.2888743281364441
Epoch 11: train_loss=0.4325118362903595


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep_combo2/checkpoints/tft-epoch=01-val_loss=0.1386.ckpt
Best validation loss: 0.138556
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 1.8 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 9.8 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.8 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 4.3 K 
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 4.3 K 
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 4.3 K 
10 | static_context_enrichment          | GatedResidualNetwork            | 4.3 K 
11 | lstm_encoder                       | LSTM                            | 8.4 K 
12 | lstm_decoder                       | LSTM                            | 8.4 K 
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.1 K 
14 | post_lstm_add_norm_encoder         | AddNorm                         | 64    
15 | static_enrichment                  | GatedResidualNetwork            | 5.3 K 
16 | multihead_attn                     | InterpretableMultiHeadAttention | 3.2 K 
17 | post_attn_gate_norm                | GateAddNorm                     | 2.2 K 
18 | pos_wise_ff                        | GatedResidualNetwork            | 4.3 K 
19 | pre_output_gate_norm               | GateAddNorm                     | 2.2 K 
20 | output_layer                       | Linear                          | 231   
----------------------------------------------------------------------------------------
67.0 K    Trainable params
0         Non-trainable params
67.0 K    Total params
0.268     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep_combo3
======================================================================
Configuration saved to: experiments/sweep_combo3/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 93

Initializing model...
Model parameters: 66,975

Starting training...
Checkpoints will be saved to: experiments/sweep_combo3/checkpoints/
Logs will be saved to: experiments/sweep_combo3/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.14779241383075714
Epoch 0: train_loss=0.6683214902877808
Epoch 1: val_loss=0.16577398777008057
Epoch 1: train_loss=0.49589627981185913
Epoch 2: val_loss=0.18700328469276428
Epoch 2: train_loss=0.4587866961956024
Epoch 3: val_loss=0.19128550589084625
Epoch 3: train_loss=0.45379647612571716
Epoch 4: val_loss=0.21612724661827087
Epoch 4: train_loss=0.44681787490844727
Epoch 5: val_loss=0.22497522830963135
Epoch 5: train_loss=0.44217613339424133
Epoch 6: val_loss=0.23842349648475647
Epoch 6: train_loss=0.4411383271217346
Epoch 7: val_loss=0.2405293881893158
Epoch 7: train_loss=0.4386362135410309
Epoch 8: val_loss=0.2364802211523056
Epoch 8: train_loss=0.43704015016555786
Epoch 9: val_loss=0.23610340058803558
Epoch 9: train_loss=0.43484193086624146
Epoch 10: val_loss=0.23378556966781616
Epoch 10: train_loss=0.4348674416542053


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep_combo3/checkpoints/tft-epoch=00-val_loss=0.1478.ckpt
Best validation loss: 0.147792
Sweep completed at Thu Oct 16 20:01:55 MST 2025
