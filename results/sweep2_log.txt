nohup: ignoring input
Starting targeted sweep at Thu Oct 16 22:16:49 MST 2025
Phase 1: Testing smaller hidden sizes...
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 424   
5  | encoder_variable_selection         | VariableSelectionNetwork        | 2.7 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 424   
7  | static_context_variable_selection  | GatedResidualNetwork            | 304   
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 304   
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 304   
10 | static_context_enrichment          | GatedResidualNetwork            | 304   
11 | lstm_encoder                       | LSTM                            | 576   
12 | lstm_decoder                       | LSTM                            | 576   
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 144   
14 | post_lstm_add_norm_encoder         | AddNorm                         | 16    
15 | static_enrichment                  | GatedResidualNetwork            | 368   
16 | multihead_attn                     | InterpretableMultiHeadAttention | 212   
17 | post_attn_gate_norm                | GateAddNorm                     | 160   
18 | pos_wise_ff                        | GatedResidualNetwork            | 304   
19 | pre_output_gate_norm               | GateAddNorm                     | 160   
20 | output_layer                       | Linear                          | 63    
----------------------------------------------------------------------------------------
7.3 K     Trainable params
0         Non-trainable params
7.3 K     Total params
0.029     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep2_hidden_8
======================================================================
Configuration saved to: experiments/sweep2_hidden_8/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 7,291

Starting training...
Checkpoints will be saved to: experiments/sweep2_hidden_8/checkpoints/
Logs will be saved to: experiments/sweep2_hidden_8/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.4045550525188446
Epoch 0: train_loss=0.7838805913925171
Epoch 1: val_loss=0.2628083825111389
Epoch 1: train_loss=0.6928580403327942
Epoch 2: val_loss=0.17793461680412292
Epoch 2: train_loss=0.6059839129447937
Epoch 3: val_loss=0.15769638121128082
Epoch 3: train_loss=0.5433987379074097
Epoch 4: val_loss=0.1451360285282135
Epoch 4: train_loss=0.5046696066856384
Epoch 5: val_loss=0.14952406287193298
Epoch 5: train_loss=0.4813096523284912
Epoch 6: val_loss=0.1608903706073761
Epoch 6: train_loss=0.46823084354400635
Epoch 7: val_loss=0.16971644759178162
Epoch 7: train_loss=0.4609411060810089
Epoch 8: val_loss=0.17667585611343384
Epoch 8: train_loss=0.4559425115585327
Epoch 9: val_loss=0.183180570602417
Epoch 9: train_loss=0.45430639386177063
Epoch 10: val_loss=0.1859941929578781
Epoch 10: train_loss=0.45275112986564636
Epoch 11: val_loss=0.18607200682163239
Epoch 11: train_loss=0.45416662096977234
Epoch 12: val_loss=0.19277936220169067
Epoch 12: train_loss=0.45321184396743774
Epoch 13: val_loss=0.19246354699134827
Epoch 13: train_loss=0.45352697372436523
Epoch 14: val_loss=0.1964975893497467
Epoch 14: train_loss=0.4532131552696228


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep2_hidden_8/checkpoints/tft-epoch=04-val_loss=0.1451.ckpt
Best validation loss: 0.145136
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 582   
5  | encoder_variable_selection         | VariableSelectionNetwork        | 3.5 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 582   
7  | static_context_variable_selection  | GatedResidualNetwork            | 460   
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 460   
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 460   
10 | static_context_enrichment          | GatedResidualNetwork            | 460   
11 | lstm_encoder                       | LSTM                            | 880   
12 | lstm_decoder                       | LSTM                            | 880   
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 220   
14 | post_lstm_add_norm_encoder         | AddNorm                         | 20    
15 | static_enrichment                  | GatedResidualNetwork            | 560   
16 | multihead_attn                     | InterpretableMultiHeadAttention | 325   
17 | post_attn_gate_norm                | GateAddNorm                     | 240   
18 | pos_wise_ff                        | GatedResidualNetwork            | 460   
19 | pre_output_gate_norm               | GateAddNorm                     | 240   
20 | output_layer                       | Linear                          | 77    
----------------------------------------------------------------------------------------
10.4 K    Trainable params
0         Non-trainable params
10.4 K    Total params
0.041     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep2_hidden_10
======================================================================
Configuration saved to: experiments/sweep2_hidden_10/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 10,354

Starting training...
Checkpoints will be saved to: experiments/sweep2_hidden_10/checkpoints/
Logs will be saved to: experiments/sweep2_hidden_10/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.7849104404449463
Epoch 0: train_loss=1.0986357927322388
Epoch 1: val_loss=0.633995771408081
Epoch 1: train_loss=0.9936502575874329
Epoch 2: val_loss=0.3472634553909302
Epoch 2: train_loss=0.8124245405197144
Epoch 3: val_loss=0.1228998675942421
Epoch 3: train_loss=0.6105939745903015
Epoch 4: val_loss=0.13028734922409058
Epoch 4: train_loss=0.5124824047088623
Epoch 5: val_loss=0.14925256371498108
Epoch 5: train_loss=0.4851338267326355
Epoch 6: val_loss=0.16216132044792175
Epoch 6: train_loss=0.46979668736457825
Epoch 7: val_loss=0.17371101677417755
Epoch 7: train_loss=0.46289029717445374
Epoch 8: val_loss=0.17807398736476898
Epoch 8: train_loss=0.45738205313682556
Epoch 9: val_loss=0.17972946166992188
Epoch 9: train_loss=0.45323458313941956
Epoch 10: val_loss=0.19047246873378754
Epoch 10: train_loss=0.4517919719219208
Epoch 11: val_loss=0.1983591765165329
Epoch 11: train_loss=0.45075708627700806
Epoch 12: val_loss=0.2058590054512024
Epoch 12: train_loss=0.4471575915813446
Epoch 13: val_loss=0.20856539905071259
Epoch 13: train_loss=0.4461178779602051


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep2_hidden_10/checkpoints/tft-epoch=03-val_loss=0.1229.ckpt
Best validation loss: 0.122900
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 764   
5  | encoder_variable_selection         | VariableSelectionNetwork        | 4.4 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 764   
7  | static_context_variable_selection  | GatedResidualNetwork            | 648   
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 648   
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 648   
10 | static_context_enrichment          | GatedResidualNetwork            | 648   
11 | lstm_encoder                       | LSTM                            | 1.2 K 
12 | lstm_decoder                       | LSTM                            | 1.2 K 
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 312   
14 | post_lstm_add_norm_encoder         | AddNorm                         | 24    
15 | static_enrichment                  | GatedResidualNetwork            | 792   
16 | multihead_attn                     | InterpretableMultiHeadAttention | 462   
17 | post_attn_gate_norm                | GateAddNorm                     | 336   
18 | pos_wise_ff                        | GatedResidualNetwork            | 648   
19 | pre_output_gate_norm               | GateAddNorm                     | 336   
20 | output_layer                       | Linear                          | 91    
----------------------------------------------------------------------------------------
14.0 K    Trainable params
0         Non-trainable params
14.0 K    Total params
0.056     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep2_hidden_12
======================================================================
Configuration saved to: experiments/sweep2_hidden_12/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 13,985

Starting training...
Checkpoints will be saved to: experiments/sweep2_hidden_12/checkpoints/
Logs will be saved to: experiments/sweep2_hidden_12/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.6325209140777588
Epoch 0: train_loss=1.033109188079834
Epoch 1: val_loss=0.4568215012550354
Epoch 1: train_loss=0.8917386531829834
Epoch 2: val_loss=0.20285636186599731
Epoch 2: train_loss=0.7128558158874512
Epoch 3: val_loss=0.11193826794624329
Epoch 3: train_loss=0.5690853595733643
Epoch 4: val_loss=0.15327107906341553
Epoch 4: train_loss=0.4949098825454712
Epoch 5: val_loss=0.1734771877527237
Epoch 5: train_loss=0.4701831042766571
Epoch 6: val_loss=0.17003636062145233
Epoch 6: train_loss=0.4631611704826355
Epoch 7: val_loss=0.17726083099842072
Epoch 7: train_loss=0.45956411957740784
Epoch 8: val_loss=0.18333041667938232
Epoch 8: train_loss=0.4561315178871155
Epoch 9: val_loss=0.18502795696258545
Epoch 9: train_loss=0.45504623651504517
Epoch 10: val_loss=0.19006472826004028
Epoch 10: train_loss=0.45519354939460754
Epoch 11: val_loss=0.19392846524715424
Epoch 11: train_loss=0.453243613243103
Epoch 12: val_loss=0.20032060146331787
Epoch 12: train_loss=0.4535035789012909
Epoch 13: val_loss=0.2103097140789032
Epoch 13: train_loss=0.4512621760368347


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep2_hidden_12/checkpoints/tft-epoch=03-val_loss=0.1119.ckpt
Best validation loss: 0.111938
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 970   
5  | encoder_variable_selection         | VariableSelectionNetwork        | 5.4 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 970   
7  | static_context_variable_selection  | GatedResidualNetwork            | 868   
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 868   
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 868   
10 | static_context_enrichment          | GatedResidualNetwork            | 868   
11 | lstm_encoder                       | LSTM                            | 1.7 K 
12 | lstm_decoder                       | LSTM                            | 1.7 K 
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 420   
14 | post_lstm_add_norm_encoder         | AddNorm                         | 28    
15 | static_enrichment                  | GatedResidualNetwork            | 1.1 K 
16 | multihead_attn                     | InterpretableMultiHeadAttention | 623   
17 | post_attn_gate_norm                | GateAddNorm                     | 448   
18 | pos_wise_ff                        | GatedResidualNetwork            | 868   
19 | pre_output_gate_norm               | GateAddNorm                     | 448   
20 | output_layer                       | Linear                          | 105   
----------------------------------------------------------------------------------------
18.2 K    Trainable params
0         Non-trainable params
18.2 K    Total params
0.073     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep2_hidden_14
======================================================================
Configuration saved to: experiments/sweep2_hidden_14/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 18,184

Starting training...
Checkpoints will be saved to: experiments/sweep2_hidden_14/checkpoints/
Logs will be saved to: experiments/sweep2_hidden_14/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.2165278196334839
Epoch 0: train_loss=0.773918092250824
Epoch 1: val_loss=0.13769084215164185
Epoch 1: train_loss=0.6589035391807556
Epoch 2: val_loss=0.13887739181518555
Epoch 2: train_loss=0.5577394962310791
Epoch 3: val_loss=0.15127792954444885
Epoch 3: train_loss=0.5034065842628479
Epoch 4: val_loss=0.15922103822231293
Epoch 4: train_loss=0.4773706793785095
Epoch 5: val_loss=0.1707853525876999
Epoch 5: train_loss=0.4647725820541382
Epoch 6: val_loss=0.17949210107326508
Epoch 6: train_loss=0.4582318663597107
Epoch 7: val_loss=0.17660364508628845
Epoch 7: train_loss=0.45659196376800537
Epoch 8: val_loss=0.18213781714439392
Epoch 8: train_loss=0.4551183879375458
Epoch 9: val_loss=0.18622440099716187
Epoch 9: train_loss=0.454802930355072
Epoch 10: val_loss=0.1896926909685135
Epoch 10: train_loss=0.4521442651748657
Epoch 11: val_loss=0.19631806015968323
Epoch 11: train_loss=0.45153558254241943


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep2_hidden_14/checkpoints/tft-epoch=01-val_loss=0.1377.ckpt
Best validation loss: 0.137691
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 1.2 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 6.4 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.2 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K 
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K 
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K 
10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K 
11 | lstm_encoder                       | LSTM                            | 2.2 K 
12 | lstm_decoder                       | LSTM                            | 2.2 K 
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   
14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    
15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K 
16 | multihead_attn                     | InterpretableMultiHeadAttention | 808   
17 | post_attn_gate_norm                | GateAddNorm                     | 576   
18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K 
19 | pre_output_gate_norm               | GateAddNorm                     | 576   
20 | output_layer                       | Linear                          | 119   
----------------------------------------------------------------------------------------
22.6 K    Trainable params
0         Non-trainable params
22.6 K    Total params
0.090     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep2_hidden_16
======================================================================
Configuration saved to: experiments/sweep2_hidden_16/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 22,615

Starting training...
Checkpoints will be saved to: experiments/sweep2_hidden_16/checkpoints/
Logs will be saved to: experiments/sweep2_hidden_16/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.3859482407569885
Epoch 0: train_loss=0.8888093829154968
Epoch 1: val_loss=0.13244730234146118
Epoch 1: train_loss=0.6538586020469666
Epoch 2: val_loss=0.15113747119903564
Epoch 2: train_loss=0.5082658529281616
Epoch 3: val_loss=0.1772746443748474
Epoch 3: train_loss=0.4686850607395172
Epoch 4: val_loss=0.1867443174123764
Epoch 4: train_loss=0.45871999859809875
Epoch 5: val_loss=0.19207289814949036
Epoch 5: train_loss=0.45595797896385193
Epoch 6: val_loss=0.18732020258903503
Epoch 6: train_loss=0.45375433564186096
Epoch 7: val_loss=0.19324584305286407
Epoch 7: train_loss=0.44986245036125183
Epoch 8: val_loss=0.2022390067577362
Epoch 8: train_loss=0.4503166079521179
Epoch 9: val_loss=0.20807136595249176
Epoch 9: train_loss=0.4487007260322571
Epoch 10: val_loss=0.21224766969680786
Epoch 10: train_loss=0.4468279778957367
Epoch 11: val_loss=0.21922841668128967
Epoch 11: train_loss=0.44511479139328003


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep2_hidden_16/checkpoints/tft-epoch=01-val_loss=0.1324.ckpt
Best validation loss: 0.132447
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 1.3 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 7.0 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.3 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 1.4 K 
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.4 K 
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.4 K 
10 | static_context_enrichment          | GatedResidualNetwork            | 1.4 K 
11 | lstm_encoder                       | LSTM                            | 2.7 K 
12 | lstm_decoder                       | LSTM                            | 2.7 K 
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 684   
14 | post_lstm_add_norm_encoder         | AddNorm                         | 36    
15 | static_enrichment                  | GatedResidualNetwork            | 1.7 K 
16 | multihead_attn                     | InterpretableMultiHeadAttention | 1.0 K 
17 | post_attn_gate_norm                | GateAddNorm                     | 720   
18 | pos_wise_ff                        | GatedResidualNetwork            | 1.4 K 
19 | pre_output_gate_norm               | GateAddNorm                     | 720   
20 | output_layer                       | Linear                          | 133   
----------------------------------------------------------------------------------------
27.1 K    Trainable params
0         Non-trainable params
27.1 K    Total params
0.108     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep2_hidden_18
======================================================================
Configuration saved to: experiments/sweep2_hidden_18/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 27,054

Starting training...
Checkpoints will be saved to: experiments/sweep2_hidden_18/checkpoints/
Logs will be saved to: experiments/sweep2_hidden_18/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.20192693173885345
Epoch 0: train_loss=0.6712175607681274
Epoch 1: val_loss=0.16466420888900757
Epoch 1: train_loss=0.5829195380210876
Epoch 2: val_loss=0.15305368602275848
Epoch 2: train_loss=0.5186727046966553
Epoch 3: val_loss=0.14766889810562134
Epoch 3: train_loss=0.4794943332672119
Epoch 4: val_loss=0.1604384183883667
Epoch 4: train_loss=0.4635072946548462
Epoch 5: val_loss=0.17717452347278595
Epoch 5: train_loss=0.45472100377082825
Epoch 6: val_loss=0.19532260298728943
Epoch 6: train_loss=0.4473482370376587
Epoch 7: val_loss=0.22108086943626404
Epoch 7: train_loss=0.4423382580280304
Epoch 8: val_loss=0.2391984462738037
Epoch 8: train_loss=0.4394601583480835
Epoch 9: val_loss=0.24375151097774506
Epoch 9: train_loss=0.43749991059303284
Epoch 10: val_loss=0.24691201746463776
Epoch 10: train_loss=0.43640923500061035
Epoch 11: val_loss=0.2544904947280884
Epoch 11: train_loss=0.4360370635986328
Epoch 12: val_loss=0.25783759355545044
Epoch 12: train_loss=0.434478223323822
Epoch 13: val_loss=0.2603413462638855
Epoch 13: train_loss=0.4339878261089325


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep2_hidden_18/checkpoints/tft-epoch=03-val_loss=0.1477.ckpt
Best validation loss: 0.147669
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 1.4 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 7.4 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.4 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 1.7 K 
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.7 K 
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.7 K 
10 | static_context_enrichment          | GatedResidualNetwork            | 1.7 K 
11 | lstm_encoder                       | LSTM                            | 3.4 K 
12 | lstm_decoder                       | LSTM                            | 3.4 K 
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 840   
14 | post_lstm_add_norm_encoder         | AddNorm                         | 40    
15 | static_enrichment                  | GatedResidualNetwork            | 2.1 K 
16 | multihead_attn                     | InterpretableMultiHeadAttention | 1.2 K 
17 | post_attn_gate_norm                | GateAddNorm                     | 880   
18 | pos_wise_ff                        | GatedResidualNetwork            | 1.7 K 
19 | pre_output_gate_norm               | GateAddNorm                     | 880   
20 | output_layer                       | Linear                          | 147   
----------------------------------------------------------------------------------------
31.6 K    Trainable params
0         Non-trainable params
31.6 K    Total params
0.126     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep2_hidden_20
======================================================================
Configuration saved to: experiments/sweep2_hidden_20/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 31,557

Starting training...
Checkpoints will be saved to: experiments/sweep2_hidden_20/checkpoints/
Logs will be saved to: experiments/sweep2_hidden_20/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.2615676820278168
Epoch 0: train_loss=0.8318939805030823
Epoch 1: val_loss=0.06711447238922119
Epoch 1: train_loss=0.6396315097808838
Epoch 2: val_loss=0.13620713353157043
Epoch 2: train_loss=0.5168099403381348
Epoch 3: val_loss=0.16767768561840057
Epoch 3: train_loss=0.4707210958003998
Epoch 4: val_loss=0.18471111357212067
Epoch 4: train_loss=0.4581340551376343
Epoch 5: val_loss=0.18859679996967316
Epoch 5: train_loss=0.4511701166629791
Epoch 6: val_loss=0.2089725285768509
Epoch 6: train_loss=0.4474465847015381
Epoch 7: val_loss=0.22453463077545166
Epoch 7: train_loss=0.4440545439720154
Epoch 8: val_loss=0.23030689358711243
Epoch 8: train_loss=0.44429001212120056
Epoch 9: val_loss=0.24086210131645203
Epoch 9: train_loss=0.4421223998069763
Epoch 10: val_loss=0.24105142056941986
Epoch 10: train_loss=0.441559761762619
Epoch 11: val_loss=0.25136828422546387
Epoch 11: train_loss=0.4399511516094208


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep2_hidden_20/checkpoints/tft-epoch=01-val_loss=0.0671.ckpt
Best validation loss: 0.067114
Phase 2: Testing learning rates with hidden_16...
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 1.2 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 6.4 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.2 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K 
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K 
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K 
10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K 
11 | lstm_encoder                       | LSTM                            | 2.2 K 
12 | lstm_decoder                       | LSTM                            | 2.2 K 
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   
14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    
15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K 
16 | multihead_attn                     | InterpretableMultiHeadAttention | 808   
17 | post_attn_gate_norm                | GateAddNorm                     | 576   
18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K 
19 | pre_output_gate_norm               | GateAddNorm                     | 576   
20 | output_layer                       | Linear                          | 119   
----------------------------------------------------------------------------------------
22.6 K    Trainable params
0         Non-trainable params
22.6 K    Total params
0.090     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep2_h16_lr_0.0002
======================================================================
Configuration saved to: experiments/sweep2_h16_lr_0.0002/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 22,615

Starting training...
Checkpoints will be saved to: experiments/sweep2_h16_lr_0.0002/checkpoints/
Logs will be saved to: experiments/sweep2_h16_lr_0.0002/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.5816433429718018
Epoch 0: train_loss=0.932441771030426
Epoch 1: val_loss=0.34619224071502686
Epoch 1: train_loss=0.8161300420761108
Epoch 2: val_loss=0.17926549911499023
Epoch 2: train_loss=0.6702293157577515
Epoch 3: val_loss=0.115431047976017
Epoch 3: train_loss=0.5669769644737244
Epoch 4: val_loss=0.13750743865966797
Epoch 4: train_loss=0.5062440633773804
Epoch 5: val_loss=0.16163109242916107
Epoch 5: train_loss=0.47794267535209656
Epoch 6: val_loss=0.17045581340789795
Epoch 6: train_loss=0.46662771701812744
Epoch 7: val_loss=0.17699626088142395
Epoch 7: train_loss=0.46020662784576416
Epoch 8: val_loss=0.18285763263702393
Epoch 8: train_loss=0.45916682481765747
Epoch 9: val_loss=0.18627111613750458
Epoch 9: train_loss=0.45768243074417114
Epoch 10: val_loss=0.18685156106948853
Epoch 10: train_loss=0.4568210244178772
Epoch 11: val_loss=0.18669280409812927
Epoch 11: train_loss=0.4557860791683197
Epoch 12: val_loss=0.18745484948158264
Epoch 12: train_loss=0.4543672800064087
Epoch 13: val_loss=0.1900283545255661
Epoch 13: train_loss=0.45440271496772766


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep2_h16_lr_0.0002/checkpoints/tft-epoch=03-val_loss=0.1154.ckpt
Best validation loss: 0.115431
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 1.2 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 6.4 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.2 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K 
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K 
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K 
10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K 
11 | lstm_encoder                       | LSTM                            | 2.2 K 
12 | lstm_decoder                       | LSTM                            | 2.2 K 
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   
14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    
15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K 
16 | multihead_attn                     | InterpretableMultiHeadAttention | 808   
17 | post_attn_gate_norm                | GateAddNorm                     | 576   
18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K 
19 | pre_output_gate_norm               | GateAddNorm                     | 576   
20 | output_layer                       | Linear                          | 119   
----------------------------------------------------------------------------------------
22.6 K    Trainable params
0         Non-trainable params
22.6 K    Total params
0.090     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep2_h16_lr_0.0003
======================================================================
Configuration saved to: experiments/sweep2_h16_lr_0.0003/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 22,615

Starting training...
Checkpoints will be saved to: experiments/sweep2_h16_lr_0.0003/checkpoints/
Logs will be saved to: experiments/sweep2_h16_lr_0.0003/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.519417941570282
Epoch 0: train_loss=0.9176837205886841
Epoch 1: val_loss=0.2485174834728241
Epoch 1: train_loss=0.751611590385437
Epoch 2: val_loss=0.12265065312385559
Epoch 2: train_loss=0.588099479675293
Epoch 3: val_loss=0.14388687908649445
Epoch 3: train_loss=0.5064389109611511
Epoch 4: val_loss=0.1701141595840454
Epoch 4: train_loss=0.47227171063423157
Epoch 5: val_loss=0.180372416973114
Epoch 5: train_loss=0.46228882670402527
Epoch 6: val_loss=0.1803278923034668
Epoch 6: train_loss=0.45862218737602234
Epoch 7: val_loss=0.18534821271896362
Epoch 7: train_loss=0.45499345660209656
Epoch 8: val_loss=0.18856775760650635
Epoch 8: train_loss=0.4554043114185333
Epoch 9: val_loss=0.19230850040912628
Epoch 9: train_loss=0.4543002247810364
Epoch 10: val_loss=0.19329246878623962
Epoch 10: train_loss=0.4527202844619751
Epoch 11: val_loss=0.19455108046531677
Epoch 11: train_loss=0.45105627179145813
Epoch 12: val_loss=0.19724518060684204
Epoch 12: train_loss=0.44907304644584656


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep2_h16_lr_0.0003/checkpoints/tft-epoch=02-val_loss=0.1227.ckpt
Best validation loss: 0.122651
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 1.2 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 6.4 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.2 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K 
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K 
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K 
10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K 
11 | lstm_encoder                       | LSTM                            | 2.2 K 
12 | lstm_decoder                       | LSTM                            | 2.2 K 
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   
14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    
15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K 
16 | multihead_attn                     | InterpretableMultiHeadAttention | 808   
17 | post_attn_gate_norm                | GateAddNorm                     | 576   
18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K 
19 | pre_output_gate_norm               | GateAddNorm                     | 576   
20 | output_layer                       | Linear                          | 119   
----------------------------------------------------------------------------------------
22.6 K    Trainable params
0         Non-trainable params
22.6 K    Total params
0.090     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep2_h16_lr_0.0004
======================================================================
Configuration saved to: experiments/sweep2_h16_lr_0.0004/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 22,615

Starting training...
Checkpoints will be saved to: experiments/sweep2_h16_lr_0.0004/checkpoints/
Logs will be saved to: experiments/sweep2_h16_lr_0.0004/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.45415475964546204
Epoch 0: train_loss=0.9030826091766357
Epoch 1: val_loss=0.1639934629201889
Epoch 1: train_loss=0.6977397203445435
Epoch 2: val_loss=0.12982743978500366
Epoch 2: train_loss=0.5386406779289246
Epoch 3: val_loss=0.16511982679367065
Epoch 3: train_loss=0.4802391231060028
Epoch 4: val_loss=0.1818741410970688
Epoch 4: train_loss=0.4623253047466278
Epoch 5: val_loss=0.18913286924362183
Epoch 5: train_loss=0.45795542001724243
Epoch 6: val_loss=0.18580061197280884
Epoch 6: train_loss=0.4557540714740753
Epoch 7: val_loss=0.1903068721294403
Epoch 7: train_loss=0.45189568400382996
Epoch 8: val_loss=0.1960209757089615
Epoch 8: train_loss=0.4518887996673584
Epoch 9: val_loss=0.20187488198280334
Epoch 9: train_loss=0.450339674949646
Epoch 10: val_loss=0.2052099108695984
Epoch 10: train_loss=0.44857677817344666
Epoch 11: val_loss=0.20984992384910583
Epoch 11: train_loss=0.44700565934181213
Epoch 12: val_loss=0.2132454663515091
Epoch 12: train_loss=0.44492223858833313


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep2_h16_lr_0.0004/checkpoints/tft-epoch=02-val_loss=0.1298.ckpt
Best validation loss: 0.129827
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 1.2 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 6.4 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.2 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K 
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K 
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K 
10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K 
11 | lstm_encoder                       | LSTM                            | 2.2 K 
12 | lstm_decoder                       | LSTM                            | 2.2 K 
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   
14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    
15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K 
16 | multihead_attn                     | InterpretableMultiHeadAttention | 808   
17 | post_attn_gate_norm                | GateAddNorm                     | 576   
18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K 
19 | pre_output_gate_norm               | GateAddNorm                     | 576   
20 | output_layer                       | Linear                          | 119   
----------------------------------------------------------------------------------------
22.6 K    Trainable params
0         Non-trainable params
22.6 K    Total params
0.090     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep2_h16_lr_0.0005
======================================================================
Configuration saved to: experiments/sweep2_h16_lr_0.0005/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 22,615

Starting training...
Checkpoints will be saved to: experiments/sweep2_h16_lr_0.0005/checkpoints/
Logs will be saved to: experiments/sweep2_h16_lr_0.0005/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.3859482407569885
Epoch 0: train_loss=0.8888093829154968
Epoch 1: val_loss=0.13244730234146118
Epoch 1: train_loss=0.6538586020469666
Epoch 2: val_loss=0.15113747119903564
Epoch 2: train_loss=0.5082658529281616
Epoch 3: val_loss=0.1772746443748474
Epoch 3: train_loss=0.4686850607395172
Epoch 4: val_loss=0.1867443174123764
Epoch 4: train_loss=0.45871999859809875
Epoch 5: val_loss=0.19207289814949036
Epoch 5: train_loss=0.45595797896385193
Epoch 6: val_loss=0.18732020258903503
Epoch 6: train_loss=0.45375433564186096
Epoch 7: val_loss=0.19324584305286407
Epoch 7: train_loss=0.44986245036125183
Epoch 8: val_loss=0.2022390067577362
Epoch 8: train_loss=0.4503166079521179
Epoch 9: val_loss=0.20807136595249176
Epoch 9: train_loss=0.4487007260322571
Epoch 10: val_loss=0.21224766969680786
Epoch 10: train_loss=0.4468279778957367
Epoch 11: val_loss=0.21922841668128967
Epoch 11: train_loss=0.44511479139328003


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep2_h16_lr_0.0005/checkpoints/tft-epoch=01-val_loss=0.1324.ckpt
Best validation loss: 0.132447
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 1.2 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 6.4 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.2 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K 
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K 
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K 
10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K 
11 | lstm_encoder                       | LSTM                            | 2.2 K 
12 | lstm_decoder                       | LSTM                            | 2.2 K 
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   
14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    
15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K 
16 | multihead_attn                     | InterpretableMultiHeadAttention | 808   
17 | post_attn_gate_norm                | GateAddNorm                     | 576   
18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K 
19 | pre_output_gate_norm               | GateAddNorm                     | 576   
20 | output_layer                       | Linear                          | 119   
----------------------------------------------------------------------------------------
22.6 K    Trainable params
0         Non-trainable params
22.6 K    Total params
0.090     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep2_h16_lr_0.0006
======================================================================
Configuration saved to: experiments/sweep2_h16_lr_0.0006/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 22,615

Starting training...
Checkpoints will be saved to: experiments/sweep2_h16_lr_0.0006/checkpoints/
Logs will be saved to: experiments/sweep2_h16_lr_0.0006/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.3403576612472534
Epoch 0: train_loss=0.8749338388442993
Epoch 1: val_loss=0.11982454359531403
Epoch 1: train_loss=0.6187853813171387
Epoch 2: val_loss=0.1657656580209732
Epoch 2: train_loss=0.4896398186683655
Epoch 3: val_loss=0.18326333165168762
Epoch 3: train_loss=0.46329060196876526
Epoch 4: val_loss=0.18977326154708862
Epoch 4: train_loss=0.4568322002887726
Epoch 5: val_loss=0.1949533075094223
Epoch 5: train_loss=0.4544751048088074
Epoch 6: val_loss=0.19691738486289978
Epoch 6: train_loss=0.4515640139579773
Epoch 7: val_loss=0.20300093293190002
Epoch 7: train_loss=0.44742581248283386
Epoch 8: val_loss=0.213721364736557
Epoch 8: train_loss=0.44793546199798584
Epoch 9: val_loss=0.21895061433315277
Epoch 9: train_loss=0.4463403522968292
Epoch 10: val_loss=0.22531050443649292
Epoch 10: train_loss=0.44436150789260864
Epoch 11: val_loss=0.23350372910499573
Epoch 11: train_loss=0.44256868958473206


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep2_h16_lr_0.0006/checkpoints/tft-epoch=01-val_loss=0.1198.ckpt
Best validation loss: 0.119825
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 1.2 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 6.4 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.2 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K 
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K 
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K 
10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K 
11 | lstm_encoder                       | LSTM                            | 2.2 K 
12 | lstm_decoder                       | LSTM                            | 2.2 K 
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   
14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    
15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K 
16 | multihead_attn                     | InterpretableMultiHeadAttention | 808   
17 | post_attn_gate_norm                | GateAddNorm                     | 576   
18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K 
19 | pre_output_gate_norm               | GateAddNorm                     | 576   
20 | output_layer                       | Linear                          | 119   
----------------------------------------------------------------------------------------
22.6 K    Trainable params
0         Non-trainable params
22.6 K    Total params
0.090     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep2_h16_lr_0.0007
======================================================================
Configuration saved to: experiments/sweep2_h16_lr_0.0007/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 22,615

Starting training...
Checkpoints will be saved to: experiments/sweep2_h16_lr_0.0007/checkpoints/
Logs will be saved to: experiments/sweep2_h16_lr_0.0007/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.3084224462509155
Epoch 0: train_loss=0.8615650534629822
Epoch 1: val_loss=0.1187467873096466
Epoch 1: train_loss=0.5906550884246826
Epoch 2: val_loss=0.1743221879005432
Epoch 2: train_loss=0.4777211844921112
Epoch 3: val_loss=0.18846142292022705
Epoch 3: train_loss=0.46044543385505676
Epoch 4: val_loss=0.1924431025981903
Epoch 4: train_loss=0.4554644227027893
Epoch 5: val_loss=0.19987180829048157
Epoch 5: train_loss=0.45252519845962524
Epoch 6: val_loss=0.20840704441070557
Epoch 6: train_loss=0.44905340671539307
Epoch 7: val_loss=0.21259042620658875
Epoch 7: train_loss=0.44523730874061584
Epoch 8: val_loss=0.22318142652511597
Epoch 8: train_loss=0.4456237554550171
Epoch 9: val_loss=0.2313149869441986
Epoch 9: train_loss=0.4439077079296112
Epoch 10: val_loss=0.2386380434036255
Epoch 10: train_loss=0.4418286383152008
Epoch 11: val_loss=0.24582651257514954
Epoch 11: train_loss=0.4399248957633972


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep2_h16_lr_0.0007/checkpoints/tft-epoch=01-val_loss=0.1187.ckpt
Best validation loss: 0.118747
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 1.2 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 6.4 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.2 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K 
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K 
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K 
10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K 
11 | lstm_encoder                       | LSTM                            | 2.2 K 
12 | lstm_decoder                       | LSTM                            | 2.2 K 
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   
14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    
15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K 
16 | multihead_attn                     | InterpretableMultiHeadAttention | 808   
17 | post_attn_gate_norm                | GateAddNorm                     | 576   
18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K 
19 | pre_output_gate_norm               | GateAddNorm                     | 576   
20 | output_layer                       | Linear                          | 119   
----------------------------------------------------------------------------------------
22.6 K    Trainable params
0         Non-trainable params
22.6 K    Total params
0.090     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep2_h16_lr_0.0008
======================================================================
Configuration saved to: experiments/sweep2_h16_lr_0.0008/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 22,615

Starting training...
Checkpoints will be saved to: experiments/sweep2_h16_lr_0.0008/checkpoints/
Logs will be saved to: experiments/sweep2_h16_lr_0.0008/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.2761417031288147
Epoch 0: train_loss=0.8487308621406555
Epoch 1: val_loss=0.13411152362823486
Epoch 1: train_loss=0.5681067705154419
Epoch 2: val_loss=0.17914655804634094
Epoch 2: train_loss=0.4702284336090088
Epoch 3: val_loss=0.19197490811347961
Epoch 3: train_loss=0.45870867371559143
Epoch 4: val_loss=0.19633521139621735
Epoch 4: train_loss=0.45435789227485657
Epoch 5: val_loss=0.205938458442688
Epoch 5: train_loss=0.45078492164611816
Epoch 6: val_loss=0.2173844873905182
Epoch 6: train_loss=0.4472212791442871
Epoch 7: val_loss=0.2222500741481781
Epoch 7: train_loss=0.4433729350566864
Epoch 8: val_loss=0.23316307365894318
Epoch 8: train_loss=0.4436303675174713
Epoch 9: val_loss=0.24315375089645386
Epoch 9: train_loss=0.44164779782295227
Epoch 10: val_loss=0.25166210532188416
Epoch 10: train_loss=0.43948715925216675
Epoch 11: val_loss=0.25738847255706787
Epoch 11: train_loss=0.43764224648475647


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep2_h16_lr_0.0008/checkpoints/tft-epoch=01-val_loss=0.1341.ckpt
Best validation loss: 0.134112
Phase 3: Testing encoder lengths with hidden_16...
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 1.2 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 6.4 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.2 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K 
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K 
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K 
10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K 
11 | lstm_encoder                       | LSTM                            | 2.2 K 
12 | lstm_decoder                       | LSTM                            | 2.2 K 
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   
14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    
15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K 
16 | multihead_attn                     | InterpretableMultiHeadAttention | 808   
17 | post_attn_gate_norm                | GateAddNorm                     | 576   
18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K 
19 | pre_output_gate_norm               | GateAddNorm                     | 576   
20 | output_layer                       | Linear                          | 119   
----------------------------------------------------------------------------------------
22.6 K    Trainable params
0         Non-trainable params
22.6 K    Total params
0.090     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep2_h16_enc_15
======================================================================
Configuration saved to: experiments/sweep2_h16_enc_15/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 22,615

Starting training...
Checkpoints will be saved to: experiments/sweep2_h16_enc_15/checkpoints/
Logs will be saved to: experiments/sweep2_h16_enc_15/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.38665634393692017
Epoch 0: train_loss=0.887229323387146
Epoch 1: val_loss=0.1319243311882019
Epoch 1: train_loss=0.6534602046012878
Epoch 2: val_loss=0.1502639353275299
Epoch 2: train_loss=0.509941577911377
Epoch 3: val_loss=0.18135449290275574
Epoch 3: train_loss=0.4690592885017395
Epoch 4: val_loss=0.18187691271305084
Epoch 4: train_loss=0.45919477939605713
Epoch 5: val_loss=0.19614791870117188
Epoch 5: train_loss=0.45577341318130493
Epoch 6: val_loss=0.19185563921928406
Epoch 6: train_loss=0.4526143968105316
Epoch 7: val_loss=0.19692900776863098
Epoch 7: train_loss=0.45016685128211975
Epoch 8: val_loss=0.20784473419189453
Epoch 8: train_loss=0.44878900051116943
Epoch 9: val_loss=0.21026511490345
Epoch 9: train_loss=0.44793182611465454
Epoch 10: val_loss=0.21856391429901123
Epoch 10: train_loss=0.4456962049007416
Epoch 11: val_loss=0.22359326481819153
Epoch 11: train_loss=0.4443555474281311


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep2_h16_enc_15/checkpoints/tft-epoch=01-val_loss=0.1319.ckpt
Best validation loss: 0.131924
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 1.2 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 6.4 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.2 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K 
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K 
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K 
10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K 
11 | lstm_encoder                       | LSTM                            | 2.2 K 
12 | lstm_decoder                       | LSTM                            | 2.2 K 
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   
14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    
15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K 
16 | multihead_attn                     | InterpretableMultiHeadAttention | 808   
17 | post_attn_gate_norm                | GateAddNorm                     | 576   
18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K 
19 | pre_output_gate_norm               | GateAddNorm                     | 576   
20 | output_layer                       | Linear                          | 119   
----------------------------------------------------------------------------------------
22.6 K    Trainable params
0         Non-trainable params
22.6 K    Total params
0.090     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep2_h16_enc_20
======================================================================
Configuration saved to: experiments/sweep2_h16_enc_20/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 22,615

Starting training...
Checkpoints will be saved to: experiments/sweep2_h16_enc_20/checkpoints/
Logs will be saved to: experiments/sweep2_h16_enc_20/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.3859482407569885
Epoch 0: train_loss=0.8888093829154968
Epoch 1: val_loss=0.13244730234146118
Epoch 1: train_loss=0.6538586020469666
Epoch 2: val_loss=0.15113747119903564
Epoch 2: train_loss=0.5082658529281616
Epoch 3: val_loss=0.1772746443748474
Epoch 3: train_loss=0.4686850607395172
Epoch 4: val_loss=0.1867443174123764
Epoch 4: train_loss=0.45871999859809875
Epoch 5: val_loss=0.19207289814949036
Epoch 5: train_loss=0.45595797896385193
Epoch 6: val_loss=0.18732020258903503
Epoch 6: train_loss=0.45375433564186096
Epoch 7: val_loss=0.19324584305286407
Epoch 7: train_loss=0.44986245036125183
Epoch 8: val_loss=0.2022390067577362
Epoch 8: train_loss=0.4503166079521179
Epoch 9: val_loss=0.20807136595249176
Epoch 9: train_loss=0.4487007260322571
Epoch 10: val_loss=0.21224766969680786
Epoch 10: train_loss=0.4468279778957367
Epoch 11: val_loss=0.21922841668128967
Epoch 11: train_loss=0.44511479139328003


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep2_h16_enc_20/checkpoints/tft-epoch=01-val_loss=0.1324.ckpt
Best validation loss: 0.132447
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 1.2 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 6.4 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.2 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K 
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K 
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K 
10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K 
11 | lstm_encoder                       | LSTM                            | 2.2 K 
12 | lstm_decoder                       | LSTM                            | 2.2 K 
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   
14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    
15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K 
16 | multihead_attn                     | InterpretableMultiHeadAttention | 808   
17 | post_attn_gate_norm                | GateAddNorm                     | 576   
18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K 
19 | pre_output_gate_norm               | GateAddNorm                     | 576   
20 | output_layer                       | Linear                          | 119   
----------------------------------------------------------------------------------------
22.6 K    Trainable params
0         Non-trainable params
22.6 K    Total params
0.090     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep2_h16_enc_30
======================================================================
Configuration saved to: experiments/sweep2_h16_enc_30/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 22,615

Starting training...
Checkpoints will be saved to: experiments/sweep2_h16_enc_30/checkpoints/
Logs will be saved to: experiments/sweep2_h16_enc_30/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.3883476257324219
Epoch 0: train_loss=0.8912635445594788
Epoch 1: val_loss=0.13037872314453125
Epoch 1: train_loss=0.6550834774971008
Epoch 2: val_loss=0.14950628578662872
Epoch 2: train_loss=0.5111556649208069
Epoch 3: val_loss=0.17833489179611206
Epoch 3: train_loss=0.4689006805419922
Epoch 4: val_loss=0.18416878581047058
Epoch 4: train_loss=0.4590483009815216
Epoch 5: val_loss=0.18876168131828308
Epoch 5: train_loss=0.4570222795009613
Epoch 6: val_loss=0.19470497965812683
Epoch 6: train_loss=0.4537654519081116
Epoch 7: val_loss=0.20074857771396637
Epoch 7: train_loss=0.45014047622680664
Epoch 8: val_loss=0.2041826993227005
Epoch 8: train_loss=0.4489497244358063
Epoch 9: val_loss=0.2115846872329712
Epoch 9: train_loss=0.4475865364074707
Epoch 10: val_loss=0.21391931176185608
Epoch 10: train_loss=0.44562506675720215
Epoch 11: val_loss=0.22866857051849365
Epoch 11: train_loss=0.44371697306632996


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep2_h16_enc_30/checkpoints/tft-epoch=01-val_loss=0.1304.ckpt
Best validation loss: 0.130379
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 1.2 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 6.4 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.2 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K 
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K 
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K 
10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K 
11 | lstm_encoder                       | LSTM                            | 2.2 K 
12 | lstm_decoder                       | LSTM                            | 2.2 K 
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   
14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    
15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K 
16 | multihead_attn                     | InterpretableMultiHeadAttention | 808   
17 | post_attn_gate_norm                | GateAddNorm                     | 576   
18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K 
19 | pre_output_gate_norm               | GateAddNorm                     | 576   
20 | output_layer                       | Linear                          | 119   
----------------------------------------------------------------------------------------
22.6 K    Trainable params
0         Non-trainable params
22.6 K    Total params
0.090     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep2_h16_enc_40
======================================================================
Configuration saved to: experiments/sweep2_h16_enc_40/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 22,615

Starting training...
Checkpoints will be saved to: experiments/sweep2_h16_enc_40/checkpoints/
Logs will be saved to: experiments/sweep2_h16_enc_40/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.39107686281204224
Epoch 0: train_loss=0.889769434928894
Epoch 1: val_loss=0.1276702582836151
Epoch 1: train_loss=0.6563177108764648
Epoch 2: val_loss=0.15163150429725647
Epoch 2: train_loss=0.5118815302848816
Epoch 3: val_loss=0.16932350397109985
Epoch 3: train_loss=0.4680924415588379
Epoch 4: val_loss=0.18197166919708252
Epoch 4: train_loss=0.4598240554332733
Epoch 5: val_loss=0.19552180171012878
Epoch 5: train_loss=0.4566892683506012
Epoch 6: val_loss=0.19498372077941895
Epoch 6: train_loss=0.45274510979652405
Epoch 7: val_loss=0.1967814564704895
Epoch 7: train_loss=0.4513323903083801
Epoch 8: val_loss=0.20278942584991455
Epoch 8: train_loss=0.4491836726665497
Epoch 9: val_loss=0.21159230172634125
Epoch 9: train_loss=0.44835561513900757
Epoch 10: val_loss=0.21636062860488892
Epoch 10: train_loss=0.446021169424057
Epoch 11: val_loss=0.22868219017982483
Epoch 11: train_loss=0.44488775730133057


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep2_h16_enc_40/checkpoints/tft-epoch=01-val_loss=0.1277.ckpt
Best validation loss: 0.127670
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 1.2 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 6.4 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.2 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K 
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K 
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K 
10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K 
11 | lstm_encoder                       | LSTM                            | 2.2 K 
12 | lstm_decoder                       | LSTM                            | 2.2 K 
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   
14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    
15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K 
16 | multihead_attn                     | InterpretableMultiHeadAttention | 808   
17 | post_attn_gate_norm                | GateAddNorm                     | 576   
18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K 
19 | pre_output_gate_norm               | GateAddNorm                     | 576   
20 | output_layer                       | Linear                          | 119   
----------------------------------------------------------------------------------------
22.6 K    Trainable params
0         Non-trainable params
22.6 K    Total params
0.090     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep2_h16_enc_50
======================================================================
Configuration saved to: experiments/sweep2_h16_enc_50/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 22,615

Starting training...
Checkpoints will be saved to: experiments/sweep2_h16_enc_50/checkpoints/
Logs will be saved to: experiments/sweep2_h16_enc_50/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.38943761587142944
Epoch 0: train_loss=0.891914963722229
Epoch 1: val_loss=0.13141195476055145
Epoch 1: train_loss=0.655211865901947
Epoch 2: val_loss=0.1525210291147232
Epoch 2: train_loss=0.5098636746406555
Epoch 3: val_loss=0.1738639771938324
Epoch 3: train_loss=0.4691561460494995
Epoch 4: val_loss=0.1830308735370636
Epoch 4: train_loss=0.4600037634372711
Epoch 5: val_loss=0.19264361262321472
Epoch 5: train_loss=0.4557350277900696
Epoch 6: val_loss=0.19218647480010986
Epoch 6: train_loss=0.45486384630203247
Epoch 7: val_loss=0.19365698099136353
Epoch 7: train_loss=0.45175060629844666
Epoch 8: val_loss=0.19617226719856262
Epoch 8: train_loss=0.45120856165885925
Epoch 9: val_loss=0.2019300013780594
Epoch 9: train_loss=0.44956353306770325
Epoch 10: val_loss=0.21050497889518738
Epoch 10: train_loss=0.446906179189682
Epoch 11: val_loss=0.2173272967338562
Epoch 11: train_loss=0.4462507963180542


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep2_h16_enc_50/checkpoints/tft-epoch=01-val_loss=0.1314.ckpt
Best validation loss: 0.131412
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 1.2 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 6.4 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.2 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K 
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K 
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K 
10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K 
11 | lstm_encoder                       | LSTM                            | 2.2 K 
12 | lstm_decoder                       | LSTM                            | 2.2 K 
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   
14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    
15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K 
16 | multihead_attn                     | InterpretableMultiHeadAttention | 808   
17 | post_attn_gate_norm                | GateAddNorm                     | 576   
18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K 
19 | pre_output_gate_norm               | GateAddNorm                     | 576   
20 | output_layer                       | Linear                          | 119   
----------------------------------------------------------------------------------------
22.6 K    Trainable params
0         Non-trainable params
22.6 K    Total params
0.090     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep2_h16_enc_60
======================================================================
Configuration saved to: experiments/sweep2_h16_enc_60/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 93

Initializing model...
Model parameters: 22,615

Starting training...
Checkpoints will be saved to: experiments/sweep2_h16_enc_60/checkpoints/
Logs will be saved to: experiments/sweep2_h16_enc_60/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.40098437666893005
Epoch 0: train_loss=0.892204761505127
Epoch 1: val_loss=0.13566820323467255
Epoch 1: train_loss=0.6600189805030823
Epoch 2: val_loss=0.15099990367889404
Epoch 2: train_loss=0.5124786496162415
Epoch 3: val_loss=0.17627976834774017
Epoch 3: train_loss=0.46970704197883606
Epoch 4: val_loss=0.18178708851337433
Epoch 4: train_loss=0.45988795161247253
Epoch 5: val_loss=0.19387008249759674
Epoch 5: train_loss=0.45768049359321594
Epoch 6: val_loss=0.19848564267158508
Epoch 6: train_loss=0.4526178240776062
Epoch 7: val_loss=0.1990966498851776
Epoch 7: train_loss=0.4496757388114929
Epoch 8: val_loss=0.20805230736732483
Epoch 8: train_loss=0.4493010640144348
Epoch 9: val_loss=0.20599108934402466
Epoch 9: train_loss=0.44680967926979065
Epoch 10: val_loss=0.21780097484588623
Epoch 10: train_loss=0.4465199410915375
Epoch 11: val_loss=0.21532052755355835
Epoch 11: train_loss=0.4454958140850067


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep2_h16_enc_60/checkpoints/tft-epoch=01-val_loss=0.1357.ckpt
Best validation loss: 0.135668
Phase 4: Testing dropout with hidden_16...
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 1.2 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 6.4 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.2 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K 
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K 
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K 
10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K 
11 | lstm_encoder                       | LSTM                            | 2.2 K 
12 | lstm_decoder                       | LSTM                            | 2.2 K 
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   
14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    
15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K 
16 | multihead_attn                     | InterpretableMultiHeadAttention | 808   
17 | post_attn_gate_norm                | GateAddNorm                     | 576   
18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K 
19 | pre_output_gate_norm               | GateAddNorm                     | 576   
20 | output_layer                       | Linear                          | 119   
----------------------------------------------------------------------------------------
22.6 K    Trainable params
0         Non-trainable params
22.6 K    Total params
0.090     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep2_h16_drop_0.05
======================================================================
Configuration saved to: experiments/sweep2_h16_drop_0.05/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 22,615

Starting training...
Checkpoints will be saved to: experiments/sweep2_h16_drop_0.05/checkpoints/
Logs will be saved to: experiments/sweep2_h16_drop_0.05/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.37937289476394653
Epoch 0: train_loss=0.8849416971206665
Epoch 1: val_loss=0.1387777030467987
Epoch 1: train_loss=0.6389096975326538
Epoch 2: val_loss=0.15493348240852356
Epoch 2: train_loss=0.497591108083725
Epoch 3: val_loss=0.17850540578365326
Epoch 3: train_loss=0.4638758599758148
Epoch 4: val_loss=0.18397167325019836
Epoch 4: train_loss=0.45703360438346863
Epoch 5: val_loss=0.18958111107349396
Epoch 5: train_loss=0.45399555563926697
Epoch 6: val_loss=0.19271767139434814
Epoch 6: train_loss=0.4528142511844635
Epoch 7: val_loss=0.20122858881950378
Epoch 7: train_loss=0.44820070266723633
Epoch 8: val_loss=0.21422410011291504
Epoch 8: train_loss=0.44803181290626526
Epoch 9: val_loss=0.22155697643756866
Epoch 9: train_loss=0.4469479024410248
Epoch 10: val_loss=0.2288585752248764
Epoch 10: train_loss=0.4454839527606964
Epoch 11: val_loss=0.23568180203437805
Epoch 11: train_loss=0.4437699019908905


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep2_h16_drop_0.05/checkpoints/tft-epoch=01-val_loss=0.1388.ckpt
Best validation loss: 0.138778
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 1.2 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 6.4 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.2 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K 
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K 
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K 
10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K 
11 | lstm_encoder                       | LSTM                            | 2.2 K 
12 | lstm_decoder                       | LSTM                            | 2.2 K 
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   
14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    
15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K 
16 | multihead_attn                     | InterpretableMultiHeadAttention | 808   
17 | post_attn_gate_norm                | GateAddNorm                     | 576   
18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K 
19 | pre_output_gate_norm               | GateAddNorm                     | 576   
20 | output_layer                       | Linear                          | 119   
----------------------------------------------------------------------------------------
22.6 K    Trainable params
0         Non-trainable params
22.6 K    Total params
0.090     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep2_h16_drop_0.1
======================================================================
Configuration saved to: experiments/sweep2_h16_drop_0.1/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 22,615

Starting training...
Checkpoints will be saved to: experiments/sweep2_h16_drop_0.1/checkpoints/
Logs will be saved to: experiments/sweep2_h16_drop_0.1/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.3822252154350281
Epoch 0: train_loss=0.886943519115448
Epoch 1: val_loss=0.1321549415588379
Epoch 1: train_loss=0.6466691493988037
Epoch 2: val_loss=0.1540251076221466
Epoch 2: train_loss=0.5028825998306274
Epoch 3: val_loss=0.1777195781469345
Epoch 3: train_loss=0.4664146602153778
Epoch 4: val_loss=0.1881781816482544
Epoch 4: train_loss=0.45786115527153015
Epoch 5: val_loss=0.1892736852169037
Epoch 5: train_loss=0.45502495765686035
Epoch 6: val_loss=0.19149214029312134
Epoch 6: train_loss=0.4533100426197052
Epoch 7: val_loss=0.19860023260116577
Epoch 7: train_loss=0.4486943781375885
Epoch 8: val_loss=0.2072451263666153
Epoch 8: train_loss=0.44940826296806335
Epoch 9: val_loss=0.21496012806892395
Epoch 9: train_loss=0.44782355427742004
Epoch 10: val_loss=0.2207636684179306
Epoch 10: train_loss=0.4457453787326813
Epoch 11: val_loss=0.22814789414405823
Epoch 11: train_loss=0.445000022649765


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep2_h16_drop_0.1/checkpoints/tft-epoch=01-val_loss=0.1322.ckpt
Best validation loss: 0.132155
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 1.2 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 6.4 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.2 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K 
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K 
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K 
10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K 
11 | lstm_encoder                       | LSTM                            | 2.2 K 
12 | lstm_decoder                       | LSTM                            | 2.2 K 
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   
14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    
15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K 
16 | multihead_attn                     | InterpretableMultiHeadAttention | 808   
17 | post_attn_gate_norm                | GateAddNorm                     | 576   
18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K 
19 | pre_output_gate_norm               | GateAddNorm                     | 576   
20 | output_layer                       | Linear                          | 119   
----------------------------------------------------------------------------------------
22.6 K    Trainable params
0         Non-trainable params
22.6 K    Total params
0.090     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep2_h16_drop_0.15
======================================================================
Configuration saved to: experiments/sweep2_h16_drop_0.15/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 22,615

Starting training...
Checkpoints will be saved to: experiments/sweep2_h16_drop_0.15/checkpoints/
Logs will be saved to: experiments/sweep2_h16_drop_0.15/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.3859482407569885
Epoch 0: train_loss=0.8888093829154968
Epoch 1: val_loss=0.13244730234146118
Epoch 1: train_loss=0.6538586020469666
Epoch 2: val_loss=0.15113747119903564
Epoch 2: train_loss=0.5082658529281616
Epoch 3: val_loss=0.1772746443748474
Epoch 3: train_loss=0.4686850607395172
Epoch 4: val_loss=0.1867443174123764
Epoch 4: train_loss=0.45871999859809875
Epoch 5: val_loss=0.19207289814949036
Epoch 5: train_loss=0.45595797896385193
Epoch 6: val_loss=0.18732020258903503
Epoch 6: train_loss=0.45375433564186096
Epoch 7: val_loss=0.19324584305286407
Epoch 7: train_loss=0.44986245036125183
Epoch 8: val_loss=0.2022390067577362
Epoch 8: train_loss=0.4503166079521179
Epoch 9: val_loss=0.20807136595249176
Epoch 9: train_loss=0.4487007260322571
Epoch 10: val_loss=0.21224766969680786
Epoch 10: train_loss=0.4468279778957367
Epoch 11: val_loss=0.21922841668128967
Epoch 11: train_loss=0.44511479139328003


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep2_h16_drop_0.15/checkpoints/tft-epoch=01-val_loss=0.1324.ckpt
Best validation loss: 0.132447
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 1.2 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 6.4 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.2 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K 
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K 
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K 
10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K 
11 | lstm_encoder                       | LSTM                            | 2.2 K 
12 | lstm_decoder                       | LSTM                            | 2.2 K 
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   
14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    
15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K 
16 | multihead_attn                     | InterpretableMultiHeadAttention | 808   
17 | post_attn_gate_norm                | GateAddNorm                     | 576   
18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K 
19 | pre_output_gate_norm               | GateAddNorm                     | 576   
20 | output_layer                       | Linear                          | 119   
----------------------------------------------------------------------------------------
22.6 K    Trainable params
0         Non-trainable params
22.6 K    Total params
0.090     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep2_h16_drop_0.2
======================================================================
Configuration saved to: experiments/sweep2_h16_drop_0.2/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 22,615

Starting training...
Checkpoints will be saved to: experiments/sweep2_h16_drop_0.2/checkpoints/
Logs will be saved to: experiments/sweep2_h16_drop_0.2/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.3900676667690277
Epoch 0: train_loss=0.8898607492446899
Epoch 1: val_loss=0.12843942642211914
Epoch 1: train_loss=0.6615973114967346
Epoch 2: val_loss=0.1486995369195938
Epoch 2: train_loss=0.513710081577301
Epoch 3: val_loss=0.1758100837469101
Epoch 3: train_loss=0.4713299572467804
Epoch 4: val_loss=0.18591560423374176
Epoch 4: train_loss=0.45980575680732727
Epoch 5: val_loss=0.19272643327713013
Epoch 5: train_loss=0.45682674646377563
Epoch 6: val_loss=0.18917188048362732
Epoch 6: train_loss=0.4539061188697815
Epoch 7: val_loss=0.19338181614875793
Epoch 7: train_loss=0.4499400556087494
Epoch 8: val_loss=0.2003655731678009
Epoch 8: train_loss=0.4504649043083191
Epoch 9: val_loss=0.20686125755310059
Epoch 9: train_loss=0.4489535391330719
Epoch 10: val_loss=0.21103090047836304
Epoch 10: train_loss=0.4475175440311432
Epoch 11: val_loss=0.2180159091949463
Epoch 11: train_loss=0.44571739435195923


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep2_h16_drop_0.2/checkpoints/tft-epoch=01-val_loss=0.1284.ckpt
Best validation loss: 0.128439
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 1.2 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 6.4 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.2 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K 
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K 
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K 
10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K 
11 | lstm_encoder                       | LSTM                            | 2.2 K 
12 | lstm_decoder                       | LSTM                            | 2.2 K 
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   
14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    
15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K 
16 | multihead_attn                     | InterpretableMultiHeadAttention | 808   
17 | post_attn_gate_norm                | GateAddNorm                     | 576   
18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K 
19 | pre_output_gate_norm               | GateAddNorm                     | 576   
20 | output_layer                       | Linear                          | 119   
----------------------------------------------------------------------------------------
22.6 K    Trainable params
0         Non-trainable params
22.6 K    Total params
0.090     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep2_h16_drop_0.25
======================================================================
Configuration saved to: experiments/sweep2_h16_drop_0.25/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 22,615

Starting training...
Checkpoints will be saved to: experiments/sweep2_h16_drop_0.25/checkpoints/
Logs will be saved to: experiments/sweep2_h16_drop_0.25/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.39605554938316345
Epoch 0: train_loss=0.8909553289413452
Epoch 1: val_loss=0.12599900364875793
Epoch 1: train_loss=0.670978844165802
Epoch 2: val_loss=0.14631521701812744
Epoch 2: train_loss=0.5197114944458008
Epoch 3: val_loss=0.17276504635810852
Epoch 3: train_loss=0.4741871654987335
Epoch 4: val_loss=0.18409030139446259
Epoch 4: train_loss=0.460574209690094
Epoch 5: val_loss=0.18936282396316528
Epoch 5: train_loss=0.4571947455406189
Epoch 6: val_loss=0.19258835911750793
Epoch 6: train_loss=0.4548622965812683
Epoch 7: val_loss=0.19253146648406982
Epoch 7: train_loss=0.4504964351654053
Epoch 8: val_loss=0.19755509495735168
Epoch 8: train_loss=0.45138272643089294
Epoch 9: val_loss=0.20344312489032745
Epoch 9: train_loss=0.4495735168457031
Epoch 10: val_loss=0.2100256085395813
Epoch 10: train_loss=0.4482124447822571
Epoch 11: val_loss=0.21757414937019348
Epoch 11: train_loss=0.4461354911327362


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep2_h16_drop_0.25/checkpoints/tft-epoch=01-val_loss=0.1260.ckpt
Best validation loss: 0.125999
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 1.2 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 6.4 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.2 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K 
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K 
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K 
10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K 
11 | lstm_encoder                       | LSTM                            | 2.2 K 
12 | lstm_decoder                       | LSTM                            | 2.2 K 
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   
14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    
15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K 
16 | multihead_attn                     | InterpretableMultiHeadAttention | 808   
17 | post_attn_gate_norm                | GateAddNorm                     | 576   
18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K 
19 | pre_output_gate_norm               | GateAddNorm                     | 576   
20 | output_layer                       | Linear                          | 119   
----------------------------------------------------------------------------------------
22.6 K    Trainable params
0         Non-trainable params
22.6 K    Total params
0.090     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep2_h16_drop_0.3
======================================================================
Configuration saved to: experiments/sweep2_h16_drop_0.3/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 22,615

Starting training...
Checkpoints will be saved to: experiments/sweep2_h16_drop_0.3/checkpoints/
Logs will be saved to: experiments/sweep2_h16_drop_0.3/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.40385299921035767
Epoch 0: train_loss=0.8921477198600769
Epoch 1: val_loss=0.1279943436384201
Epoch 1: train_loss=0.6783527135848999
Epoch 2: val_loss=0.14335677027702332
Epoch 2: train_loss=0.5256661176681519
Epoch 3: val_loss=0.17061130702495575
Epoch 3: train_loss=0.4767357110977173
Epoch 4: val_loss=0.18279679119586945
Epoch 4: train_loss=0.46233266592025757
Epoch 5: val_loss=0.18782521784305573
Epoch 5: train_loss=0.45746520161628723
Epoch 6: val_loss=0.18886008858680725
Epoch 6: train_loss=0.4556612968444824
Epoch 7: val_loss=0.19060423970222473
Epoch 7: train_loss=0.45126423239707947
Epoch 8: val_loss=0.19648751616477966
Epoch 8: train_loss=0.4517359137535095
Epoch 9: val_loss=0.20131751894950867
Epoch 9: train_loss=0.44997742772102356
Epoch 10: val_loss=0.2065720111131668
Epoch 10: train_loss=0.4490742087364197
Epoch 11: val_loss=0.2121063619852066
Epoch 11: train_loss=0.44721677899360657


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep2_h16_drop_0.3/checkpoints/tft-epoch=01-val_loss=0.1280.ckpt
Best validation loss: 0.127994
Phase 5: Testing promising combinations...
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 764   
5  | encoder_variable_selection         | VariableSelectionNetwork        | 4.4 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 764   
7  | static_context_variable_selection  | GatedResidualNetwork            | 648   
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 648   
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 648   
10 | static_context_enrichment          | GatedResidualNetwork            | 648   
11 | lstm_encoder                       | LSTM                            | 1.2 K 
12 | lstm_decoder                       | LSTM                            | 1.2 K 
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 312   
14 | post_lstm_add_norm_encoder         | AddNorm                         | 24    
15 | static_enrichment                  | GatedResidualNetwork            | 792   
16 | multihead_attn                     | InterpretableMultiHeadAttention | 462   
17 | post_attn_gate_norm                | GateAddNorm                     | 336   
18 | pos_wise_ff                        | GatedResidualNetwork            | 648   
19 | pre_output_gate_norm               | GateAddNorm                     | 336   
20 | output_layer                       | Linear                          | 91    
----------------------------------------------------------------------------------------
14.0 K    Trainable params
0         Non-trainable params
14.0 K    Total params
0.056     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep2_combo1
======================================================================
Configuration saved to: experiments/sweep2_combo1/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 13,985

Starting training...
Checkpoints will be saved to: experiments/sweep2_combo1/checkpoints/
Logs will be saved to: experiments/sweep2_combo1/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.6471115350723267
Epoch 0: train_loss=1.0384728908538818
Epoch 1: val_loss=0.5193058848381042
Epoch 1: train_loss=0.9292207956314087
Epoch 2: val_loss=0.2877780497074127
Epoch 2: train_loss=0.7782642245292664
Epoch 3: val_loss=0.1424729824066162
Epoch 3: train_loss=0.6354774832725525
Epoch 4: val_loss=0.12528583407402039
Epoch 4: train_loss=0.5351365804672241
Epoch 5: val_loss=0.16257160902023315
Epoch 5: train_loss=0.4901033043861389
Epoch 6: val_loss=0.17172637581825256
Epoch 6: train_loss=0.47049587965011597
Epoch 7: val_loss=0.17578142881393433
Epoch 7: train_loss=0.46386682987213135
Epoch 8: val_loss=0.17790737748146057
Epoch 8: train_loss=0.4601197838783264
Epoch 9: val_loss=0.18273887038230896
Epoch 9: train_loss=0.45798254013061523
Epoch 10: val_loss=0.18486103415489197
Epoch 10: train_loss=0.45573028922080994
Epoch 11: val_loss=0.18184852600097656
Epoch 11: train_loss=0.45682916045188904
Epoch 12: val_loss=0.18611150979995728
Epoch 12: train_loss=0.45546719431877136
Epoch 13: val_loss=0.19563336670398712
Epoch 13: train_loss=0.4544909596443176
Epoch 14: val_loss=0.19628527760505676
Epoch 14: train_loss=0.4539583921432495


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep2_combo1/checkpoints/tft-epoch=04-val_loss=0.1253.ckpt
Best validation loss: 0.125286
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 582   
5  | encoder_variable_selection         | VariableSelectionNetwork        | 3.5 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 582   
7  | static_context_variable_selection  | GatedResidualNetwork            | 460   
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 460   
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 460   
10 | static_context_enrichment          | GatedResidualNetwork            | 460   
11 | lstm_encoder                       | LSTM                            | 880   
12 | lstm_decoder                       | LSTM                            | 880   
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 220   
14 | post_lstm_add_norm_encoder         | AddNorm                         | 20    
15 | static_enrichment                  | GatedResidualNetwork            | 560   
16 | multihead_attn                     | InterpretableMultiHeadAttention | 325   
17 | post_attn_gate_norm                | GateAddNorm                     | 240   
18 | pos_wise_ff                        | GatedResidualNetwork            | 460   
19 | pre_output_gate_norm               | GateAddNorm                     | 240   
20 | output_layer                       | Linear                          | 77    
----------------------------------------------------------------------------------------
10.4 K    Trainable params
0         Non-trainable params
10.4 K    Total params
0.041     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep2_combo2
======================================================================
Configuration saved to: experiments/sweep2_combo2/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 10,354

Starting training...
Checkpoints will be saved to: experiments/sweep2_combo2/checkpoints/
Logs will be saved to: experiments/sweep2_combo2/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.7847957611083984
Epoch 0: train_loss=1.09766685962677
Epoch 1: val_loss=0.6352171897888184
Epoch 1: train_loss=0.994629979133606
Epoch 2: val_loss=0.3496103584766388
Epoch 2: train_loss=0.8113711476325989
Epoch 3: val_loss=0.12488682568073273
Epoch 3: train_loss=0.6098654866218567
Epoch 4: val_loss=0.13072848320007324
Epoch 4: train_loss=0.5133262276649475
Epoch 5: val_loss=0.15059319138526917
Epoch 5: train_loss=0.48336777091026306
Epoch 6: val_loss=0.16154369711875916
Epoch 6: train_loss=0.4698949456214905
Epoch 7: val_loss=0.1747397780418396
Epoch 7: train_loss=0.4629335105419159
Epoch 8: val_loss=0.17279639840126038
Epoch 8: train_loss=0.45694753527641296
Epoch 9: val_loss=0.1813250631093979
Epoch 9: train_loss=0.4546816349029541
Epoch 10: val_loss=0.18899545073509216
Epoch 10: train_loss=0.451539009809494
Epoch 11: val_loss=0.19508615136146545
Epoch 11: train_loss=0.45000186562538147
Epoch 12: val_loss=0.20404189825057983
Epoch 12: train_loss=0.44738420844078064
Epoch 13: val_loss=0.209492027759552
Epoch 13: train_loss=0.44696053862571716


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep2_combo2/checkpoints/tft-epoch=03-val_loss=0.1249.ckpt
Best validation loss: 0.124887
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

   | Name                               | Type                            | Params
----------------------------------------------------------------------------------------
0  | loss                               | QuantileLoss                    | 0     
1  | logging_metrics                    | ModuleList                      | 0     
2  | input_embeddings                   | MultiEmbedding                  | 0     
3  | prescalers                         | ModuleDict                      | 192   
4  | static_variable_selection          | VariableSelectionNetwork        | 1.3 K 
5  | encoder_variable_selection         | VariableSelectionNetwork        | 7.0 K 
6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.3 K 
7  | static_context_variable_selection  | GatedResidualNetwork            | 1.4 K 
8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.4 K 
9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.4 K 
10 | static_context_enrichment          | GatedResidualNetwork            | 1.4 K 
11 | lstm_encoder                       | LSTM                            | 2.7 K 
12 | lstm_decoder                       | LSTM                            | 2.7 K 
13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 684   
14 | post_lstm_add_norm_encoder         | AddNorm                         | 36    
15 | static_enrichment                  | GatedResidualNetwork            | 1.7 K 
16 | multihead_attn                     | InterpretableMultiHeadAttention | 1.0 K 
17 | post_attn_gate_norm                | GateAddNorm                     | 720   
18 | pos_wise_ff                        | GatedResidualNetwork            | 1.4 K 
19 | pre_output_gate_norm               | GateAddNorm                     | 720   
20 | output_layer                       | Linear                          | 133   
----------------------------------------------------------------------------------------
27.1 K    Trainable params
0         Non-trainable params
27.1 K    Total params
0.108     Total estimated model params size (MB)

[INFO] Patched matplotlib.Axes.errorbar to abs() negative yerr for robustness.

[INFO] Using AUTO device selection.
======================================================================
Training TFT: sweep2_combo3
======================================================================
Configuration saved to: experiments/sweep2_combo3/config.json

Loading data splits...
Train: 6066 samples
Val: 1300 samples
Test: 1300 samples

Preparing TimeSeriesDataSet...
Batches per epoch: 94

Initializing model...
Model parameters: 27,054

Starting training...
Checkpoints will be saved to: experiments/sweep2_combo3/checkpoints/
Logs will be saved to: experiments/sweep2_combo3/logs/

Epoch 0: val_loss=N/A
Epoch 0: val_loss=0.21612179279327393
Epoch 0: train_loss=0.6848922371864319
Epoch 1: val_loss=0.18472497165203094
Epoch 1: train_loss=0.6250324845314026
Epoch 2: val_loss=0.1591070145368576
Epoch 2: train_loss=0.567215621471405
Epoch 3: val_loss=0.14089302718639374
Epoch 3: train_loss=0.5236049294471741
Epoch 4: val_loss=0.14233064651489258
Epoch 4: train_loss=0.49462395906448364
Epoch 5: val_loss=0.15335746109485626
Epoch 5: train_loss=0.4767847955226898
Epoch 6: val_loss=0.16278210282325745
Epoch 6: train_loss=0.46621865034103394
Epoch 7: val_loss=0.16805924475193024
Epoch 7: train_loss=0.45907196402549744
Epoch 8: val_loss=0.17536672949790955
Epoch 8: train_loss=0.4559004306793213
Epoch 9: val_loss=0.1793786883354187
Epoch 9: train_loss=0.45034101605415344
Epoch 10: val_loss=0.18364955484867096
Epoch 10: train_loss=0.44969984889030457
Epoch 11: val_loss=0.19024527072906494
Epoch 11: train_loss=0.447600781917572
Epoch 12: val_loss=0.1973205804824829
Epoch 12: train_loss=0.4459215998649597
Epoch 13: val_loss=0.20477062463760376
Epoch 13: train_loss=0.44431841373443604


======================================================================
Training complete!
======================================================================

Best model checkpoint: experiments/sweep2_combo3/checkpoints/tft-epoch=03-val_loss=0.1409.ckpt
Best validation loss: 0.140893
Targeted sweep completed at Thu Oct 16 23:17:26 MST 2025

Total experiments: ~30
Run analysis with: python scripts/analyze_sweep.py
